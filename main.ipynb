{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7402218,"sourceType":"datasetVersion","datasetId":4304373}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nfrom transformers import BertTokenizer, BertModel, AutoModel, AutoProcessor\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch, torchaudio, torchtext\nfrom torcheval.metrics.functional import multiclass_f1_score\nimport torch.nn as nn\nimport os\nimport gc\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:32:10.920767Z","iopub.execute_input":"2024-01-24T16:32:10.921163Z","iopub.status.idle":"2024-01-24T16:32:11.367419Z","shell.execute_reply.started":"2024-01-24T16:32:10.921134Z","shell.execute_reply":"2024-01-24T16:32:11.366373Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load df","metadata":{}},{"cell_type":"code","source":"try:\n    df_path = '/kaggle/input/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = '/kaggle/input/MM-USElecDeb60to16/audio_clips'\n    save_path = '/kaggle/working/'\n    df = pd.read_csv(df_path, index_col=0)\nexcept FileNotFoundError:\n    df_path = 'multimodal-dataset/files/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = 'multimodal-dataset/files/MM-USElecDeb60to16/audio_clips'\n    save_path = 'multimodal-dataset/files'\n    df = pd.read_csv(df_path, index_col=0)\n# drop rows where audio length is 0\ndf = df[df['NewBegin'] != df['NewEnd']]\n\ntrain_df_complete = df[df['Set'] == 'TRAIN']\nval_df_complete = df[df['Set'] == 'VALIDATION']\ntest_df_complete = df[df['Set'] == 'TEST']\n\nDATASET_RATIO = 0.40\n\ntrain_df = train_df_complete.iloc[:int(DATASET_RATIO * len(train_df_complete))]\nval_df = val_df_complete.iloc[:int(DATASET_RATIO * len(val_df_complete))]\ntest_df = test_df_complete.iloc[:int(DATASET_RATIO * len(test_df_complete))]","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:45:56.623329Z","iopub.execute_input":"2024-01-24T14:45:56.624603Z","iopub.status.idle":"2024-01-24T14:45:56.950448Z","shell.execute_reply.started":"2024-01-24T14:45:56.624564Z","shell.execute_reply":"2024-01-24T14:45:56.949497Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:01.002455Z","iopub.execute_input":"2024-01-24T14:46:01.002827Z","iopub.status.idle":"2024-01-24T14:46:01.034504Z","shell.execute_reply.started":"2024-01-24T14:46:01.002798Z","shell.execute_reply":"2024-01-24T14:46:01.033464Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                Text  Part Document  Order  \\\n0  CHENEY: Gwen, I want to thank you, and I want ...     1  30_2004      0   \n1  It's a very important event, and they've done ...     1  30_2004      1   \n2  It's important to look at all of our developme...     1  30_2004      2   \n3  And, after 9/11, it became clear that we had t...     1  30_2004      3   \n4  And we also then finally had to stand up democ...     1  30_2004      4   \n\n   Sentence  Start   End  Annotator                                   Tag  \\\n0         0   2101  2221        NaN                             {\"O\": 27}   \n1         1   2221  2304        NaN                             {\"O\": 19}   \n2         2   2304  2418        NaN                             {\"O\": 23}   \n3         3   2418  2744        NaN                {\"O\": 16, \"Claim\": 50}   \n4         4   2744  2974        NaN  {\"O\": 4, \"Claim\": 13, \"Premise\": 25}   \n\n  Component  ... Speaker SpeakerType    Set         Date  Year  \\\n0         O  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n1         O  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n2         O  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n3     Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n4   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n\n                      Name MainTag NewBegin  NewEnd  idClip  \n0  Richard(Dick) B. Cheney       O   126.52  131.08  clip_0  \n1  Richard(Dick) B. Cheney       O   131.08  134.40  clip_1  \n2  Richard(Dick) B. Cheney       O   134.40  140.56  clip_2  \n3  Richard(Dick) B. Cheney   Claim   140.56  158.92  clip_3  \n4  Richard(Dick) B. Cheney   Mixed   158.92  172.92  clip_4  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Part</th>\n      <th>Document</th>\n      <th>Order</th>\n      <th>Sentence</th>\n      <th>Start</th>\n      <th>End</th>\n      <th>Annotator</th>\n      <th>Tag</th>\n      <th>Component</th>\n      <th>...</th>\n      <th>Speaker</th>\n      <th>SpeakerType</th>\n      <th>Set</th>\n      <th>Date</th>\n      <th>Year</th>\n      <th>Name</th>\n      <th>MainTag</th>\n      <th>NewBegin</th>\n      <th>NewEnd</th>\n      <th>idClip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHENEY: Gwen, I want to thank you, and I want ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2101</td>\n      <td>2221</td>\n      <td>NaN</td>\n      <td>{\"O\": 27}</td>\n      <td>O</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>O</td>\n      <td>126.52</td>\n      <td>131.08</td>\n      <td>clip_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>It's a very important event, and they've done ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2221</td>\n      <td>2304</td>\n      <td>NaN</td>\n      <td>{\"O\": 19}</td>\n      <td>O</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>O</td>\n      <td>131.08</td>\n      <td>134.40</td>\n      <td>clip_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>It's important to look at all of our developme...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2304</td>\n      <td>2418</td>\n      <td>NaN</td>\n      <td>{\"O\": 23}</td>\n      <td>O</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>O</td>\n      <td>134.40</td>\n      <td>140.56</td>\n      <td>clip_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>And, after 9/11, it became clear that we had t...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2418</td>\n      <td>2744</td>\n      <td>NaN</td>\n      <td>{\"O\": 16, \"Claim\": 50}</td>\n      <td>Claim</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Claim</td>\n      <td>140.56</td>\n      <td>158.92</td>\n      <td>clip_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And we also then finally had to stand up democ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2744</td>\n      <td>2974</td>\n      <td>NaN</td>\n      <td>{\"O\": 4, \"Claim\": 13, \"Premise\": 25}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Mixed</td>\n      <td>158.92</td>\n      <td>172.92</td>\n      <td>clip_4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(train_df), len(test_df), len(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:03.074994Z","iopub.execute_input":"2024-01-24T14:46:03.075391Z","iopub.status.idle":"2024-01-24T14:46:03.085650Z","shell.execute_reply.started":"2024-01-24T14:46:03.075359Z","shell.execute_reply":"2024-01-24T14:46:03.084546Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(4967, 2986, 2758)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Distribution of classes over train df","metadata":{}},{"cell_type":"code","source":"num_claim = len(train_df[train_df['Component'] == 'Claim'])\nnum_premise = len(train_df[train_df['Component'] == 'Premise'])\nnum_other = len(train_df[train_df['Component'] == 'O'])\n\nprint(f'Total Claim: {num_claim}: {num_claim*100/len(train_df):.2f}%')\nprint(f'Total Premise: {num_premise}: {num_premise*100/len(train_df):.2f}%')\nprint(f'Total Other: {num_other}: {num_other*100/len(train_df):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T15:51:30.335051Z","iopub.execute_input":"2024-01-24T15:51:30.336084Z","iopub.status.idle":"2024-01-24T15:51:30.351234Z","shell.execute_reply.started":"2024-01-24T15:51:30.336045Z","shell.execute_reply":"2024-01-24T15:51:30.350008Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Total Claim: 2103: 42.34%\nTotal Premise: 1697: 34.17%\nTotal Other: 1167: 23.50%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train and evaluation Loop","metadata":{}},{"cell_type":"code","source":"class BestModel:\n    \"\"\"\n        Class to keep track of the best performing model on validation set during training\n    \"\"\"\n    def __init__(self):\n        self.best_validation_loss = float('Infinity')\n        self.best_state_dict = None\n    def __call__(self, model, loss):\n        if loss < self.best_validation_loss:\n            self.best_validation_loss = loss\n            self.best_state_dict = model.state_dict()\n\ndef train(model, optimizer, loss_fn, train_loader, val_loader, epochs=10, device=\"cuda\"):\n    best_model_tracker = BestModel()\n    for epoch in tqdm(range(epochs)):\n        training_loss = 0.0\n        valid_loss = 0.0\n        model.train()\n\n        for batch in train_loader:\n            optimizer.zero_grad()\n            texts, audio_features, audio_attention, targets = batch\n            audio_features = audio_features.to(device)\n            audio_attention = audio_attention.to(device)\n            targets = targets.to(device)\n            output = model(texts,audio_features,audio_attention)\n            loss = loss_fn(output, targets)\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.detach()\n        training_loss = training_loss.cpu().item()\n        training_loss /= len(train_loader.dataset)\n\n        model.eval()\n        num_correct = 0 \n        num_examples = 0\n        tot_pred, tot_targ = torch.LongTensor().to(device), torch.LongTensor().to(device)\n        for batch in val_loader:\n            texts, audio_features, audio_attention, targets = batch\n            audio_features = audio_features.to(device)\n            audio_attention = audio_attention.to(device)\n            targets = targets.to(device)\n            output = model(texts,audio_features,audio_attention)\n            loss = loss_fn(output, targets)\n            valid_loss += loss.detach()\n            predicted_labels = torch.argmax(output, dim=-1)\n            tot_targ = torch.cat((tot_targ, targets))\n            tot_pred = torch.cat((tot_pred, predicted_labels))            \n            correct = torch.eq(predicted_labels, targets).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n        best_model_tracker(model, valid_loss)\n        valid_loss = valid_loss.cpu().item()\n        valid_loss /= len(val_loader.dataset)  \n        print(f'Epoch: {epoch}, Training Loss: {training_loss:.4f}, Validation Loss: {valid_loss:.4f}, accuracy = {num_correct/num_examples:.4f}, F1={multiclass_f1_score(tot_pred, tot_targ, num_classes=3, average=\"macro\"):.4f}')\n    model.load_state_dict(best_model_tracker.best_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:36:18.191198Z","iopub.execute_input":"2024-01-24T16:36:18.191980Z","iopub.status.idle":"2024-01-24T16:36:18.207239Z","shell.execute_reply.started":"2024-01-24T16:36:18.191947Z","shell.execute_reply":"2024-01-24T16:36:18.206043Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Creation","metadata":{}},{"cell_type":"code","source":"text_model_card = 'bert-base-uncased'\naudio_model_card = 'facebook/wav2vec2-base-960h'\n\ntokenizer = BertTokenizer.from_pretrained(text_model_card)\nembedder = BertModel.from_pretrained(text_model_card).to(device)\n\nfor params in embedder.parameters():\n    params.requires_grad = False\n\nlabel_2_id = {\n    'Claim': 0,\n    'Premise': 1,\n    'O': 2\n}\n\nDOWNSAMPLE_FACTOR = 1/5\n\nclass MM_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, audio_dir, sample_rate):\n        self.audio_dir = audio_dir\n        self.sample_rate = sample_rate\n\n        self.audio_processor = AutoProcessor.from_pretrained(audio_model_card)\n        self.audio_model = AutoModel.from_pretrained(audio_model_card).to(device)\n\n        self.dataset = []\n\n        # Iterate over df\n        for _, row in tqdm(df.iterrows()):\n            path = os.path.join(self.audio_dir, f\"{row['Document']}/{row['idClip']}.wav\")\n            if os.path.exists(path):\n                # obtain audio WAV2VEC features\n                audio, sampling_rate = torchaudio.load(path)\n                if sampling_rate != self.sample_rate:\n                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\n                    audio = torch.mean(audio, dim=0, keepdim=True)\n                with torch.inference_mode():\n                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\n                    input_values = torch.tensor(input_values).to(device)\n                    audio_model_output = self.audio_model(input_values)\n                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\n                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode='linear')\n                    audio_features = audio_features.permute(0,2,1)[0]\n                    audio_features = audio_features.cpu()\n                \n                text = row['Text']\n\n                self.dataset.append((text, audio_features, label_2_id[row['Component']]))\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        return self.dataset[index]","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:11.990246Z","iopub.execute_input":"2024-01-24T14:46:11.990970Z","iopub.status.idle":"2024-01-24T14:46:15.765327Z","shell.execute_reply.started":"2024-01-24T14:46:11.990941Z","shell.execute_reply":"2024-01-24T14:46:15.764278Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb1992b44da4708968781aa0bc2d4c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a131205e49904d19806a863bfd2f2341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ed730b97f094f1297ee689cb9c6cf79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051e531ad1df4d51b6b4772ee7ada28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb2e62c162a45ab8b596f583bc0e61b"}},"metadata":{}}]},{"cell_type":"code","source":"try:\n    train_dataset = torch.load(f'{save_path}/train_dataset.pkl')\n    test_dataset = torch.load(f'{save_path}/test_dataset.pkl')\n    val_dataset = torch.load(f'{save_path}/val_dataset.pkl')\n    print('Restored datasets from memory')\nexcept:\n    print('Creating new datasets')\n    train_dataset = MM_Dataset(train_df, audio_path, 16_000)\n    test_dataset = MM_Dataset(test_df, audio_path, 16_000)\n    val_dataset = MM_Dataset(val_df, audio_path, 16_000)\n    torch.save(train_dataset, f'{save_path}/train_dataset.pkl')\n    torch.save(test_dataset, f'{save_path}/test_dataset.pkl')\n    torch.save(val_dataset, f'{save_path}/val_dataset.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:40.778080Z","iopub.execute_input":"2024-01-24T14:46:40.778499Z","iopub.status.idle":"2024-01-24T15:05:24.663127Z","shell.execute_reply.started":"2024-01-24T14:46:40.778466Z","shell.execute_reply":"2024-01-24T15:05:24.661975Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Creating new datasets\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d1ec46991b042b0bad871ba5a4a774b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3084a7620cf2481cbef466ad775a9f5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40143bd31f7a46858c0716401f95b0eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c38cf7b30854885892adb8ab4310192"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ee5b9b256b4f0e8af4401f1a4a9910"}},"metadata":{}},{"name":"stdout","text":"Ignored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"348e0ba799ef4ecca82f0483aaccac62"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n4967it [08:29,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ignored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2986it [04:25, 11.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ignored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2758it [05:18,  8.65it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataloader creation","metadata":{}},{"cell_type":"code","source":"def create_dataloader(dataset, batch_size):\n    def pack_fn(batch):\n        texts = [x[0] for x in batch]\n        audio_features = [x[1] for x in batch]\n        labels = torch.tensor([x[2] for x in batch])\n        \n        # pad audio features\n        audio_features = pad_sequence(audio_features, batch_first=True, padding_value=float('-inf'))\n\n        audio_features_attention_mask = audio_features[:, :, 0] != float('-inf')\n        \n        audio_features[(audio_features == float('-inf'))] = 0\n\n        return texts, audio_features, audio_features_attention_mask, labels\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pack_fn)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-01-24T15:09:08.277967Z","iopub.execute_input":"2024-01-24T15:09:08.278460Z","iopub.status.idle":"2024-01-24T15:09:08.285997Z","shell.execute_reply.started":"2024-01-24T15:09:08.278429Z","shell.execute_reply":"2024-01-24T15:09:08.285038Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataloader = create_dataloader(train_dataset, 8)\nval_dataloader = create_dataloader(val_dataset, 8)\ntest_dataloader = create_dataloader(test_dataset, 8)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T15:09:09.740162Z","iopub.execute_input":"2024-01-24T15:09:09.740617Z","iopub.status.idle":"2024-01-24T15:09:09.746719Z","shell.execute_reply.started":"2024-01-24T15:09:09.740585Z","shell.execute_reply":"2024-01-24T15:09:09.745527Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#del early_fusion\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T15:09:14.129935Z","iopub.execute_input":"2024-01-24T15:09:14.130659Z","iopub.status.idle":"2024-01-24T15:09:14.560366Z","shell.execute_reply.started":"2024-01-24T15:09:14.130623Z","shell.execute_reply":"2024-01-24T15:09:14.559149Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1096"},"metadata":{}}]},{"cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"cell_type":"code","source":"import math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dual_modality=False, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n        self.dual_modality = dual_modality\n        self.pe = self.pe.to(device)\n\n    def forward(self, x, is_first=True):\n        if self.dual_modality:\n            modality = torch.ones((x.shape[0], x.shape[1], 4), dtype=torch.float32).to(device) * (0 if is_first else 1)\n            x = x + self.pe[:x.size(0)]\n            x = self.dropout(x)        \n            return torch.cat((x, modality), axis=-1)\n        else:\n            x = x + self.pe[:x.size(0)]\n            return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:51:46.329674Z","iopub.execute_input":"2024-01-24T16:51:46.330038Z","iopub.status.idle":"2024-01-24T16:51:46.341526Z","shell.execute_reply.started":"2024-01-24T16:51:46.330013Z","shell.execute_reply":"2024-01-24T16:51:46.340424Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"# Multimodal-Transformer Model","metadata":{}},{"cell_type":"code","source":"class MultiModalTransformer(nn.Module):\n    def __init__(self, tokenizer, embedder, transformer, head):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(768, dual_modality=True)\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.transformer = transformer\n        self.head = head\n\n    def forward(self, texts, audio_features, audio_attentions):\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][0]\n        text_features = self.pos_encoder(text_features, is_first=True)\n        text_attentions = tokenizer_output.attention_mask\n        \n        audio_features = self.pos_encoder(audio_features, is_first=False)\n        \n        concatenated_features = torch.cat((text_features, audio_features), dim=1)\n        concatenated_attentions = torch.cat((text_attentions, audio_attentions.float()), dim=1)\n        \n        # padding mask is 1 where there is padding (i.e. where attention is 0) and 0 otherwise\n        concatenated_padding_mask = ~concatenated_attentions.to(torch.bool)\n        \n        # compute a full attention mask of size [seq_len, seq_len]\n        full_attention_mask = torch.zeros((concatenated_features.shape[1], concatenated_features.shape[1]), dtype=torch.bool).to(device)\n                \n        transformer_output = self.transformer(src=concatenated_features,  mask=full_attention_mask, src_key_padding_mask=concatenated_padding_mask)\n        transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1)\n        return self.head(transformer_output_pooled)\n\ntransformer_layer = nn.TransformerEncoderLayer(d_model=772, nhead=4, dim_feedforward=512, batch_first=True).to(device)\ntransformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4).to(device)\n\nhead = nn.Sequential(\n    nn.Linear(772, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\nmultimodal_transformer = MultiModalTransformer(tokenizer, embedder, transformer_encoder, head).to(device)\n\noptimizer = torch.optim.Adam(multimodal_transformer.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\ntrain(multimodal_transformer, optimizer, criterion, train_dataloader, val_dataloader, epochs=10, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:58:29.691184Z","iopub.execute_input":"2024-01-24T16:58:29.691962Z","iopub.status.idle":"2024-01-24T17:13:41.003968Z","shell.execute_reply.started":"2024-01-24T16:58:29.691929Z","shell.execute_reply":"2024-01-24T17:13:41.002833Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":" 10%|â–ˆ         | 1/10 [01:31<13:39, 91.08s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.1320, Validation Loss: 0.1361, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 2/10 [03:02<12:08, 91.07s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.1303, Validation Loss: 0.1335, accuracy = 0.4569, F1=0.2170\n","output_type":"stream"},{"name":"stderr","text":" 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [04:33<10:37, 91.07s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.1321, Validation Loss: 0.1340, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [06:04<09:06, 91.13s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.1336, Validation Loss: 0.1328, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [07:35<07:35, 91.08s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.1319, Validation Loss: 0.1340, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [09:06<06:04, 91.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.1334, Validation Loss: 0.1326, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [10:37<04:33, 91.10s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.1333, Validation Loss: 0.1351, accuracy = 0.3183, F1=0.1610\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [12:08<03:02, 91.10s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.1336, Validation Loss: 0.1365, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [13:40<01:31, 91.16s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.1329, Validation Loss: 0.1336, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [15:11<00:00, 91.12s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.1334, Validation Loss: 0.1355, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Ensembling-Fusion Model","metadata":{}},{"cell_type":"markdown","source":"## Text-Only and Audio-Only Models ","metadata":{}},{"cell_type":"code","source":"class TextModel(nn.Module):\n    def __init__(self, tokenizer, embedder, head):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(768, dual_modality=False)\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.head = head\n    def forward(self, texts, audio_features, audio_attention):\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['last_hidden_state']\n        text_features = self.pos_encoder(text_features)        \n        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n        return self.head(text_features_pooled)\n    \nclass AudioModel(nn.Module):        \n    def __init__(self, transformer, head):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(768, dual_modality=False)\n        self.transformer = transformer\n        self.head = head\n        \n    def forward(self, texts, audio_features, audio_attention):\n        padding_mask = ~audio_attention.to(torch.bool)\n        audio_features = self.pos_encoder(audio_features)\n        full_attention_mask = torch.zeros((audio_features.shape[1],audio_features.shape[1]), dtype=torch.bool).to(device)\n        transformer_output = self.transformer(src=audio_features, mask=full_attention_mask, src_key_padding_mask=padding_mask)\n        \n        # pooling transformer output\n        transformer_output_sum = (transformer_output * audio_attention.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / audio_attention.sum(axis=1).unsqueeze(-1)\n        return self.head(transformer_output_pooled)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:36:32.410387Z","iopub.execute_input":"2024-01-24T16:36:32.411056Z","iopub.status.idle":"2024-01-24T16:36:32.423037Z","shell.execute_reply.started":"2024-01-24T16:36:32.411023Z","shell.execute_reply":"2024-01-24T16:36:32.421940Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling Model","metadata":{}},{"cell_type":"code","source":" class EnsemblingFusion(nn.Module):\n    def __init__(self, text_model, audio_model):\n        super().__init__()\n        self.text_model = text_model\n        self.audio_model = audio_model\n        self.weight = torch.nn.Parameter(torch.tensor(0.0))\n        \n    def forward(self, texts, audio_features, audio_attentions):\n        text_logits = self.text_model(texts, audio_features, audio_attentions)\n        audio_logits = self.audio_model(texts, audio_features, audio_attentions)\n        \n        text_probabilities = torch.nn.functional.softmax(text_logits)\n        audio_probabilities = torch.nn.functional.softmax(audio_logits)\n        \n        coefficient = (torch.tanh(self.weight) + 1) / 2\n        \n        coefficient = coefficient*0.4 + 0.3\n        \n        return coefficient*text_probabilities + (1-coefficient)*audio_probabilities\n    \ntext_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\naudio_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\ntransformer_layer = nn.TransformerEncoderLayer(d_model=768, nhead=4, dim_feedforward=512, batch_first=True).to(device)\ntransformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4).to(device)\n\ntext_model = TextModel(tokenizer, embedder, text_head)\naudio_model = AudioModel(transformer_encoder, audio_head)\n\nensembling_fusion = EnsemblingFusion(text_model, audio_model)\n\noptimizer = torch.optim.Adam(ensembling_fusion.parameters(), lr=1e-4)\n\ndef custom_loss(outputs, targets):\n    return torch.nn.functional.nll_loss(torch.log(outputs), targets, reduction='mean')\n\ntrain(ensembling_fusion, optimizer, custom_loss, train_dataloader, val_dataloader, epochs=5, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:37:20.954452Z","iopub.execute_input":"2024-01-24T16:37:20.955369Z","iopub.status.idle":"2024-01-24T16:44:02.248873Z","shell.execute_reply.started":"2024-01-24T16:37:20.955336Z","shell.execute_reply":"2024-01-24T16:44:02.247734Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 1/5 [01:20<05:21, 80.28s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.1268, Validation Loss: 0.1263, accuracy = 0.4543, F1=0.2083\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [02:40<04:00, 80.23s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.1183, Validation Loss: 0.1215, accuracy = 0.5174, F1=0.4007\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [04:00<02:40, 80.14s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.1137, Validation Loss: 0.1202, accuracy = 0.5413, F1=0.4591\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:21<01:20, 80.34s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.1113, Validation Loss: 0.1191, accuracy = 0.5518, F1=0.4844\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:41<00:00, 80.24s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.1098, Validation Loss: 0.1188, accuracy = 0.5540, F1=0.4901\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text-Only","metadata":{}},{"cell_type":"code","source":"head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\ntext_only = TextModel(tokenizer, embedder, head)\n\noptimizer = torch.optim.Adam(text_only.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\ntrain(text_only, optimizer, criterion, train_dataloader, val_dataloader, epochs=5, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:32:26.778055Z","iopub.execute_input":"2024-01-24T16:32:26.778742Z","iopub.status.idle":"2024-01-24T16:35:33.950461Z","shell.execute_reply.started":"2024-01-24T16:32:26.778708Z","shell.execute_reply":"2024-01-24T16:35:33.949288Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 1/5 [00:36<02:27, 36.80s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.1243, Validation Loss: 0.1234, accuracy = 0.4833\n0.2991773784160614\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:14<01:52, 37.58s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.1142, Validation Loss: 0.1184, accuracy = 0.5370\n0.44944000244140625\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:53<01:15, 37.93s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.1090, Validation Loss: 0.1165, accuracy = 0.5544\n0.495005339384079\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:30<00:37, 37.54s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.1064, Validation Loss: 0.1160, accuracy = 0.5638\n0.511488676071167\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:07<00:00, 37.43s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.1052, Validation Loss: 0.1161, accuracy = 0.5667\n0.514333188533783\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Unaligned Multimodal Modal","metadata":{}},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n\nclass CrossModalAttentionBlock(nn.Module):\n    def __init__(self, embedding_dim, d_ffn):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.d_ffn = d_ffn\n        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n        self.mh_attention = nn.MultiheadAttention(self.embedding_dim, 4, 0.1, batch_first=True)\n        self.pointwise_ff = PositionwiseFeedForward(self.embedding_dim, d_ffn=self.d_ffn)\n    \n    def forward(self, elem_a, elem_b, attn_mask):\n        elem_a = self.layer_norm(elem_a)\n        elem_b = self.layer_norm(elem_b)\n        attn_mask = attn_mask.to(torch.float32)\n        \n        mh_out, _ = self.mh_attention(elem_a, elem_b, elem_b, key_padding_mask=attn_mask, need_weights=False)\n        add_out = mh_out + elem_a\n        \n        add_out_norm = self.layer_norm(add_out)\n        out_ffn = self.pointwise_ff(add_out_norm)\n        out = out_ffn + add_out\n        return out\n    \nclass UnalignedMultimodalModel(nn.Module):\n    def __init__(self, embedding_dim, d_ffn, n_blocks, head):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.d_ffn = d_ffn\n        self.n_blocks = n_blocks\n        self.head = head\n        self.text_crossmodal_blocks = nn.ModuleList([\n            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n        ])\n        self.audio_crossmodal_blocks = nn.ModuleList([\n            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n        ])\n        self.pos_encoder = PositionalEncoding(embedding_dim, dual_modality=False)\n    \n    def forward(self, texts, audio_features, audio_attentions):\n        tokenizer_output = tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][0]\n        text_features = self.pos_encoder(text_features)\n        text_attentions = tokenizer_output.attention_mask\n        \n        audio_features = self.pos_encoder(audio_features)\n        \n        text_crossmodal_out = text_features\n        for cm_block in self.text_crossmodal_blocks:\n            text_crossmodal_out = cm_block(text_crossmodal_out, audio_features, audio_attentions)\n        \n        audio_crossmodal_out = audio_features\n        for cm_block in self.audio_crossmodal_blocks:\n            audio_crossmodal_out = cm_block(audio_crossmodal_out, text_features, text_attentions)\n\n        text_crossmodal_out_mean = torch.mean(text_crossmodal_out, dim=1)\n        audio_crossmodal_out_mean = torch.mean(audio_crossmodal_out, dim=1)\n        \n        text_audio = torch.cat((text_crossmodal_out_mean, audio_crossmodal_out_mean), dim=-1)\n        \n        return self.head(text_audio)\n        \n        \nhead = nn.Sequential(\n    nn.Linear(768*2, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\nunaligned_mm_model = UnalignedMultimodalModel(768, 100, 4, head).to(device)\n\noptimizer = torch.optim.Adam(unaligned_mm_model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\ntrain(unaligned_mm_model, optimizer, criterion, train_dataloader, val_dataloader, epochs=5, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:27:32.297399Z","iopub.execute_input":"2024-01-24T18:27:32.298204Z","iopub.status.idle":"2024-01-24T18:33:30.700223Z","shell.execute_reply.started":"2024-01-24T18:27:32.298173Z","shell.execute_reply":"2024-01-24T18:33:30.699063Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 1/5 [01:11<04:46, 71.75s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.1258, Validation Loss: 0.1215, accuracy = 0.5279, F1=0.4592\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [02:23<03:34, 71.65s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.1098, Validation Loss: 0.1197, accuracy = 0.5558, F1=0.5068\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:34<02:23, 71.60s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.1014, Validation Loss: 0.1253, accuracy = 0.5620, F1=0.5301\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [04:46<01:11, 71.61s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.0957, Validation Loss: 0.1319, accuracy = 0.5566, F1=0.5191\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [05:58<00:00, 71.64s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.0897, Validation Loss: 0.1395, accuracy = 0.5580, F1=0.5252\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}