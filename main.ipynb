{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7402218,"sourceType":"datasetVersion","datasetId":4304373},{"sourceId":7480566,"sourceType":"datasetVersion","datasetId":4354582}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"!pip install torcheval","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:17.839518Z","iopub.execute_input":"2024-01-27T10:00:17.840092Z","iopub.status.idle":"2024-01-27T10:00:31.165608Z","shell.execute_reply.started":"2024-01-27T10:00:17.840064Z","shell.execute_reply":"2024-01-27T10:00:31.164682Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torcheval\n  Obtaining dependency information for torcheval from https://files.pythonhosted.org/packages/e4/de/e7abc784b00de9d05999657d29187f1f7a3406ed10ecaf164de06482608f/torcheval-0.0.7-py3-none-any.whl.metadata\n  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.5.0)\nDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nfrom transformers import BertTokenizer, BertModel, AutoModel, AutoProcessor\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch, torchaudio, torchtext\nfrom torcheval.metrics.functional import multiclass_f1_score\nimport torch.nn as nn\nimport os\nimport gc\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')\n\nMODEL_NUM_LABELS = 3\nREMOVE_OTHER = True\nOTHER_LABEL = 'O'\n    \nif REMOVE_OTHER:\n    MODEL_NUM_LABELS = 2","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:31.167535Z","iopub.execute_input":"2024-01-27T10:00:31.167804Z","iopub.status.idle":"2024-01-27T10:00:47.626624Z","shell.execute_reply.started":"2024-01-27T10:00:31.167781Z","shell.execute_reply":"2024-01-27T10:00:47.625659Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load df","metadata":{}},{"cell_type":"code","source":"try:\n    df_path = '/kaggle/input/multimodal-argument-mining/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = '/kaggle/input/multimodal-argument-mining/MM-USElecDeb60to16/audio_clips'\n    save_path = '/kaggle/input/mm-dataset-subsampling/'\n    df = pd.read_csv(df_path, index_col=0)\nexcept FileNotFoundError:\n    df_path = 'multimodal-dataset/files/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = 'multimodal-dataset/files/MM-USElecDeb60to16/audio_clips'\n    save_path = 'multimodal-dataset/files'\n    df = pd.read_csv(df_path, index_col=0)\n    \n# drop rows where audio length is 0\ndf = df[df['NewBegin'] != df['NewEnd']]\nif REMOVE_OTHER:\n    # drop rows where Component is 'Other'\n    df = df[df['Component'] != OTHER_LABEL]\n\ntrain_df_complete = df[df['Set'] == 'TRAIN']\nval_df_complete = df[df['Set'] == 'VALIDATION']\ntest_df_complete = df[df['Set'] == 'TEST']\n\nDATASET_RATIO = 1\n\ntrain_df = train_df_complete.iloc[:int(DATASET_RATIO * len(train_df_complete))]\nval_df = val_df_complete.iloc[:int(DATASET_RATIO * len(val_df_complete))]\ntest_df = test_df_complete.iloc[:int(DATASET_RATIO * len(test_df_complete))]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:47.627712Z","iopub.execute_input":"2024-01-27T10:00:47.628289Z","iopub.status.idle":"2024-01-27T10:00:47.946194Z","shell.execute_reply.started":"2024-01-27T10:00:47.628264Z","shell.execute_reply":"2024-01-27T10:00:47.945396Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:47.949074Z","iopub.execute_input":"2024-01-27T10:00:47.949744Z","iopub.status.idle":"2024-01-27T10:00:47.977217Z","shell.execute_reply.started":"2024-01-27T10:00:47.949706Z","shell.execute_reply":"2024-01-27T10:00:47.976346Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                 Text  Part Document  Order  \\\n3   And, after 9/11, it became clear that we had t...     1  30_2004      3   \n4   And we also then finally had to stand up democ...     1  30_2004      4   \n9   What we did in Iraq was exactly the right thin...     1  30_2004      9   \n10  If I had it to recommend all over again, I wou...     1  30_2004     10   \n11  The world is far safer today because Saddam Hu...     1  30_2004     11   \n\n    Sentence  Start   End  Annotator                                   Tag  \\\n3          3   2418  2744        NaN                {\"O\": 16, \"Claim\": 50}   \n4          4   2744  2974        NaN  {\"O\": 4, \"Claim\": 13, \"Premise\": 25}   \n9          9   3861  3916        NaN                 {\"Claim\": 12, \"O\": 1}   \n10        10   3916  4010        NaN               {\"Premise\": 19, \"O\": 1}   \n11        11   4010  4112        NaN   {\"Claim\": 6, \"O\": 2, \"Premise\": 13}   \n\n   Component  ... Speaker SpeakerType    Set         Date  Year  \\\n3      Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n4    Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n9      Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n10   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n11   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n\n                       Name  MainTag NewBegin  NewEnd   idClip  \n3   Richard(Dick) B. Cheney    Claim   140.56  158.92   clip_3  \n4   Richard(Dick) B. Cheney    Mixed   158.92  172.92   clip_4  \n9   Richard(Dick) B. Cheney    Claim   224.08  226.88   clip_9  \n10  Richard(Dick) B. Cheney  Premise   226.88  231.56  clip_10  \n11  Richard(Dick) B. Cheney    Mixed   231.56  237.56  clip_11  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Part</th>\n      <th>Document</th>\n      <th>Order</th>\n      <th>Sentence</th>\n      <th>Start</th>\n      <th>End</th>\n      <th>Annotator</th>\n      <th>Tag</th>\n      <th>Component</th>\n      <th>...</th>\n      <th>Speaker</th>\n      <th>SpeakerType</th>\n      <th>Set</th>\n      <th>Date</th>\n      <th>Year</th>\n      <th>Name</th>\n      <th>MainTag</th>\n      <th>NewBegin</th>\n      <th>NewEnd</th>\n      <th>idClip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>And, after 9/11, it became clear that we had t...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2418</td>\n      <td>2744</td>\n      <td>NaN</td>\n      <td>{\"O\": 16, \"Claim\": 50}</td>\n      <td>Claim</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Claim</td>\n      <td>140.56</td>\n      <td>158.92</td>\n      <td>clip_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And we also then finally had to stand up democ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2744</td>\n      <td>2974</td>\n      <td>NaN</td>\n      <td>{\"O\": 4, \"Claim\": 13, \"Premise\": 25}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Mixed</td>\n      <td>158.92</td>\n      <td>172.92</td>\n      <td>clip_4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>What we did in Iraq was exactly the right thin...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>9</td>\n      <td>9</td>\n      <td>3861</td>\n      <td>3916</td>\n      <td>NaN</td>\n      <td>{\"Claim\": 12, \"O\": 1}</td>\n      <td>Claim</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Claim</td>\n      <td>224.08</td>\n      <td>226.88</td>\n      <td>clip_9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>If I had it to recommend all over again, I wou...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>10</td>\n      <td>10</td>\n      <td>3916</td>\n      <td>4010</td>\n      <td>NaN</td>\n      <td>{\"Premise\": 19, \"O\": 1}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Premise</td>\n      <td>226.88</td>\n      <td>231.56</td>\n      <td>clip_10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The world is far safer today because Saddam Hu...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>11</td>\n      <td>11</td>\n      <td>4010</td>\n      <td>4112</td>\n      <td>NaN</td>\n      <td>{\"Claim\": 6, \"O\": 2, \"Premise\": 13}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Mixed</td>\n      <td>231.56</td>\n      <td>237.56</td>\n      <td>clip_11</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(train_df), len(test_df), len(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:47.978265Z","iopub.execute_input":"2024-01-27T10:00:47.978540Z","iopub.status.idle":"2024-01-27T10:00:47.984778Z","shell.execute_reply.started":"2024-01-27T10:00:47.978517Z","shell.execute_reply":"2024-01-27T10:00:47.983836Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(9455, 5908, 5201)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Distribution of classes over train df","metadata":{}},{"cell_type":"code","source":"num_claim = len(train_df[train_df['Component'] == 'Claim'])\nprint(f'Total Claim: {num_claim}: {num_claim*100/len(train_df):.2f}%')\n\nnum_premise = len(train_df[train_df['Component'] == 'Premise'])\nprint(f'Total Premise: {num_premise}: {num_premise*100/len(train_df):.2f}%')\n\nif not REMOVE_OTHER:\n    num_other = len(train_df[train_df['Component'] == 'O'])\n    print(f'Total Other: {num_other}: {num_other*100/len(train_df):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:47.986112Z","iopub.execute_input":"2024-01-27T10:00:47.986495Z","iopub.status.idle":"2024-01-27T10:00:48.003536Z","shell.execute_reply.started":"2024-01-27T10:00:47.986461Z","shell.execute_reply":"2024-01-27T10:00:48.002611Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total Claim: 5029: 53.19%\nTotal Premise: 4426: 46.81%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Unbalanced dataset","metadata":{}},{"cell_type":"markdown","source":"# Train and evaluation Loop","metadata":{}},{"cell_type":"code","source":"ce_loss = nn.CrossEntropyLoss()\n\nclass BestModel:\n    \"\"\"\n        Class to keep track of the best performing model on validation set during training\n    \"\"\"\n    def __init__(self):\n        self.best_validation_loss = float('Infinity')\n        self.best_state_dict = None\n    def __call__(self, model, loss):\n        if loss < self.best_validation_loss:\n            self.best_validation_loss = loss\n            self.best_state_dict = model.state_dict()\n\ndef evaluate(model, val_loader, loss_fn):\n    model.eval()\n    valid_loss = 0.0\n    num_correct = 0 \n    num_examples = 0\n    tot_pred, tot_targ = torch.LongTensor().to(device), torch.LongTensor().to(device)\n    for batch in val_loader:\n        texts, audio_features, audio_attention, targets = batch\n        audio_features = audio_features.to(device)\n        audio_attention = audio_attention.to(device)\n        targets = targets.to(device)\n        output = model(texts,audio_features,audio_attention)\n        # print(\"out\",output)\n        # print(\"targets\",targets)\n        loss = loss_fn(output, targets)\n        valid_loss += loss.detach()\n        \n        # if label O is still in the dataset we remove it from the outputs\n        # since it's a binary task\n        if not REMOVE_OTHER:\n            not_other = targets != 2\n            output = output[not_other]\n            targets = targets[not_other]\n        \n        predicted_labels = torch.argmax(output, dim=-1)\n        tot_targ = torch.cat((tot_targ, targets))\n        tot_pred = torch.cat((tot_pred, predicted_labels))            \n        correct = torch.eq(predicted_labels, targets).view(-1)\n        num_correct += torch.sum(correct).item()\n        num_examples += correct.shape[0]\n    valid_loss = valid_loss.cpu().item()\n    valid_loss /= len(val_loader.dataset)\n    accuracy = num_correct/num_examples\n    f1 = multiclass_f1_score(tot_pred, tot_targ, num_classes=2, average=\"macro\")\n    return valid_loss, accuracy, f1\n\n            \ndef train(model, loss_fn, train_loader, val_loader, epochs=10, device=\"cuda\", lr=1e-3, lr_decay_factor=0.1, lr_decay_patience=3, weight_decay=1e-5):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_decay_patience, verbose=True)\n    best_model_tracker = BestModel()\n    for epoch in tqdm(range(epochs)):\n        training_loss = 0.0\n        model.train()\n\n        for batch in train_loader:\n            optimizer.zero_grad()\n            texts, audio_features, audio_attention, targets = batch\n            audio_features = audio_features.to(device)\n            audio_attention = audio_attention.to(device)\n            targets = targets.to(device)\n            output = model(texts,audio_features,audio_attention)\n            loss = loss_fn(output, targets)\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.detach()\n        training_loss = training_loss.cpu().item()\n        training_loss /= len(train_loader.dataset)\n        valid_loss, accuracy, f1 = evaluate(model, val_loader, loss_fn)\n        best_model_tracker(model, valid_loss)\n        scheduler.step(valid_loss)\n        print(f'Epoch: {epoch}, Training Loss: {training_loss:.4f}, Validation Loss: {valid_loss:.4f}, accuracy = {accuracy:.4f}, F1={f1:.4f}')\n    model.load_state_dict(best_model_tracker.best_state_dict)    ","metadata":{"execution":{"iopub.status.busy":"2024-01-27T14:51:23.536054Z","iopub.execute_input":"2024-01-27T14:51:23.536935Z","iopub.status.idle":"2024-01-27T14:51:23.554624Z","shell.execute_reply.started":"2024-01-27T14:51:23.536899Z","shell.execute_reply":"2024-01-27T14:51:23.553443Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Creation","metadata":{}},{"cell_type":"code","source":"text_model_card = 'bert-base-uncased'\naudio_model_card = 'facebook/wav2vec2-base-960h'\n\ntokenizer = BertTokenizer.from_pretrained(text_model_card)\nembedder = BertModel.from_pretrained(text_model_card).to(device)\n\nfor params in embedder.parameters():\n    params.requires_grad = False\n\nlabel_2_id = {\n    'Claim': 0,\n    'Premise': 1,\n    'O': 2\n}\n\nDOWNSAMPLE_FACTOR = 1/5\n\nclass MM_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, audio_dir, sample_rate):\n        self.audio_dir = audio_dir\n        self.sample_rate = sample_rate\n\n        self.audio_processor = AutoProcessor.from_pretrained(audio_model_card)\n        self.audio_model = AutoModel.from_pretrained(audio_model_card).to(device)\n\n        self.dataset = []\n\n        # Iterate over df\n        for _, row in tqdm(df.iterrows()):\n            path = os.path.join(self.audio_dir, f\"{row['Document']}/{row['idClip']}.wav\")\n            if os.path.exists(path):\n                # obtain audio WAV2VEC features\n                audio, sampling_rate = torchaudio.load(path)\n                if sampling_rate != self.sample_rate:\n                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\n                    audio = torch.mean(audio, dim=0, keepdim=True)\n                with torch.inference_mode():\n                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\n                    input_values = torch.tensor(input_values).to(device)\n                    audio_model_output = self.audio_model(input_values)\n                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\n                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode='linear')\n                    audio_features = audio_features.permute(0,2,1)[0]\n                    audio_features = audio_features.cpu()\n                \n                text = row['Text']\n\n                self.dataset.append((text, audio_features, label_2_id[row['Component']]))\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        return self.dataset[index]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:48.023963Z","iopub.execute_input":"2024-01-27T10:00:48.024222Z","iopub.status.idle":"2024-01-27T10:00:51.722348Z","shell.execute_reply.started":"2024-01-27T10:00:48.024199Z","shell.execute_reply":"2024-01-27T10:00:51.721404Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c787eb6ca94783b20bcaaf5d0c0094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a975a88c1fb4e2da4702ade2f1617d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bade970ebf641f685dd001bcf4ca7ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b3396ccaa44e158507e704d7d04b95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"726f8a9e4bcf46cd85fd652daf8993b6"}},"metadata":{}}]},{"cell_type":"code","source":"try:\n    train_dataset = torch.load(f'{save_path}/train_dataset.pkl')\n    test_dataset = torch.load(f'{save_path}/test_dataset.pkl')\n    val_dataset = torch.load(f'{save_path}/val_dataset.pkl')\n    if REMOVE_OTHER:\n        train_dataset = list(filter(lambda x: x[2] != 2, train_dataset))\n        test_dataset = list(filter(lambda x: x[2] != 2, test_dataset))\n        val_dataset = list(filter(lambda x: x[2] != 2, val_dataset))\n    print('Restored datasets from memory')\nexcept:\n    print('Creating new datasets')\n    train_dataset = MM_Dataset(train_df, audio_path, 16_000)\n    test_dataset = MM_Dataset(test_df, audio_path, 16_000)\n    val_dataset = MM_Dataset(val_df, audio_path, 16_000)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:00:51.723583Z","iopub.execute_input":"2024-01-27T10:00:51.723871Z","iopub.status.idle":"2024-01-27T10:03:24.790701Z","shell.execute_reply.started":"2024-01-27T10:00:51.723831Z","shell.execute_reply":"2024-01-27T10:03:24.789618Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Restored datasets from memory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataloader creation","metadata":{}},{"cell_type":"code","source":"def create_dataloader(dataset, batch_size):\n    def pack_fn(batch):\n        texts = [x[0] for x in batch]\n        audio_features = [x[1] for x in batch]\n        labels = torch.tensor([x[2] for x in batch])\n        \n        # pad audio features\n        audio_features = pad_sequence(audio_features, batch_first=True, padding_value=float('-inf'))\n\n        audio_features_attention_mask = audio_features[:, :, 0] != float('-inf')\n        \n        audio_features[(audio_features == float('-inf'))] = 0\n\n        return texts, audio_features, audio_features_attention_mask, labels\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pack_fn)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:03:24.804074Z","iopub.execute_input":"2024-01-27T10:03:24.804413Z","iopub.status.idle":"2024-01-27T10:03:24.811416Z","shell.execute_reply.started":"2024-01-27T10:03:24.804383Z","shell.execute_reply":"2024-01-27T10:03:24.810519Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 12\n\ntrain_dataloader = create_dataloader(train_dataset, BATCH_SIZE)\nval_dataloader = create_dataloader(val_dataset, BATCH_SIZE)\ntest_dataloader = create_dataloader(test_dataset, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:03:24.812761Z","iopub.execute_input":"2024-01-27T10:03:24.813012Z","iopub.status.idle":"2024-01-27T10:03:24.824230Z","shell.execute_reply.started":"2024-01-27T10:03:24.812990Z","shell.execute_reply":"2024-01-27T10:03:24.823423Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:03:24.825299Z","iopub.execute_input":"2024-01-27T10:03:24.825823Z","iopub.status.idle":"2024-01-27T10:03:25.126496Z","shell.execute_reply.started":"2024-01-27T10:03:24.825798Z","shell.execute_reply":"2024-01-27T10:03:25.125597Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"18"},"metadata":{}}]},{"cell_type":"code","source":"def number_parameters(model):\n    \"\"\"\n        Computes the number of trainable parameters in the model\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:03:25.127646Z","iopub.execute_input":"2024-01-27T10:03:25.127929Z","iopub.status.idle":"2024-01-27T10:03:25.139214Z","shell.execute_reply.started":"2024-01-27T10:03:25.127907Z","shell.execute_reply":"2024-01-27T10:03:25.138406Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"cell_type":"code","source":"import math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dual_modality=False, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n        self.dual_modality = dual_modality\n        self.pe = self.pe.to(device)\n\n    def forward(self, x, is_first=True):\n        if self.dual_modality:\n            modality = torch.ones((x.shape[0], x.shape[1], 4), dtype=torch.float32).to(device) * (0 if is_first else 1)\n            x = x + self.pe[:,:x.size(1)]\n            x = self.dropout(x)\n            return torch.cat((x, modality), axis=-1)\n        else:\n            x = x + self.pe[:,:x.size(1)]\n            return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T11:51:57.456429Z","iopub.execute_input":"2024-01-27T11:51:57.456807Z","iopub.status.idle":"2024-01-27T11:51:57.467186Z","shell.execute_reply.started":"2024-01-27T11:51:57.456779Z","shell.execute_reply":"2024-01-27T11:51:57.466322Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Multimodal-Transformer","metadata":{}},{"cell_type":"code","source":"class MultiModalTransformer(nn.Module):\n    def __init__(self, tokenizer, embedder, transformer, head):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(768, dual_modality=False)\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.transformer = transformer\n        self.head = head\n\n    def forward(self, texts, audio_features, audio_attentions):\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][9]\n        \n        #text_features = self.pos_encoder(text_features, is_first=True)\n        text_attentions = tokenizer_output.attention_mask\n        #audio_features = self.pos_encoder(audio_features, is_first=False)\n        \n        test = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=text_features.size(1)/audio_features.size(1), mode='linear')\n        test = test.permute(0,2,1)\n        \n        concatenated_features = torch.cat((text_features, test), dim=1)\n        concatenated_attentions = torch.cat((text_attentions, audio_attentions.float()), dim=1)\n        \n        # padding mask is 1 where there is padding (i.e. where attention is 0) and 0 otherwise\n        concatenated_padding_mask = ~(concatenated_attentions.to(torch.bool))\n        \n        # compute a full attention mask of size [seq_len, seq_len]\n        full_attention_mask = torch.zeros((concatenated_features.shape[1], concatenated_features.shape[1]), dtype=torch.float32).to(device)\n        \n        \n        seq_len = concatenated_features.shape[1]\n        rel_pos_enc = np.fromfunction(lambda i, j: torch.sin(torch.tensor(math.pi * (i / (seq_len - 1) * (j + 1)))), (seq_len, seq_len)) \n\n        rel_pos_enc = torch.from_numpy(rel_pos_enc).float().to(device)\n                \n        transformer_output = self.transformer(src=concatenated_features, mask=rel_pos_enc)#, src_key_padding_mask=concatenated_padding_mask)\n        #print(\"to\",transformer_output)\n        #transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1)\n        #transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1)\n        #print(\"to shape\",transformer_output.shape)\n        #print(\"to_p \", torch.mean(torch.abs(transformer_output_pooled),dim=1))\n        transformer_output_pooled = torch.mean(transformer_output, dim=1)\n        \n        return self.head(transformer_output_pooled)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T15:14:30.089362Z","iopub.execute_input":"2024-01-27T15:14:30.090521Z","iopub.status.idle":"2024-01-27T15:14:30.103477Z","shell.execute_reply.started":"2024-01-27T15:14:30.090487Z","shell.execute_reply":"2024-01-27T15:14:30.102550Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"del multimodal_criterion \ndel multimodal_optimizer\ndel multimodal_transformer_layer\ndel multimodal_transformer_encoder\ndel multimodal_transformer_head\ndel multimodal_transformer\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T10:03:25.167290Z","iopub.execute_input":"2024-01-27T10:03:25.167643Z","iopub.status.idle":"2024-01-27T10:03:26.032997Z","shell.execute_reply.started":"2024-01-27T10:03:25.167611Z","shell.execute_reply":"2024-01-27T10:03:26.031815Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m multimodal_criterion \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m multimodal_optimizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m multimodal_transformer_layer\n","\u001b[0;31mNameError\u001b[0m: name 'multimodal_criterion' is not defined"],"ename":"NameError","evalue":"name 'multimodal_criterion' is not defined","output_type":"error"}]},{"cell_type":"code","source":"multimodal_transformer_layer = nn.TransformerEncoderLayer(d_model=768, nhead=2, batch_first=True).to(device)\nmultimodal_transformer_encoder = nn.TransformerEncoder(multimodal_transformer_layer, num_layers=1).to(device)\n\nmultimodal_transformer_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, MODEL_NUM_LABELS)\n).to(device)\n\nmultimodal_transformer = MultiModalTransformer(tokenizer, embedder, multimodal_transformer_encoder, multimodal_transformer_head).to(device)\n\nmultimodal_optimizer = torch.optim.Adam(multimodal_transformer.parameters(), lr=1e-3)\nmultimodal_criterion = nn.CrossEntropyLoss()\n\ntrain(multimodal_transformer, multimodal_criterion, train_dataloader, val_dataloader, epochs=10, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T15:32:01.115476Z","iopub.execute_input":"2024-01-27T15:32:01.116357Z","iopub.status.idle":"2024-01-27T16:16:22.511542Z","shell.execute_reply.started":"2024-01-27T15:32:01.116327Z","shell.execute_reply":"2024-01-27T16:16:22.510524Z"},"trusted":true},"execution_count":185,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [04:21<39:14, 261.56s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.0510, Validation Loss: 0.0690, accuracy = 0.6383, F1=0.5874\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [08:44<34:59, 262.39s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.0487, Validation Loss: 0.0553, accuracy = 0.6439, F1=0.5912\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [13:11<30:52, 264.57s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.0460, Validation Loss: 0.0569, accuracy = 0.6549, F1=0.6138\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [17:39<26:34, 265.83s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.0453, Validation Loss: 0.0576, accuracy = 0.6843, F1=0.6607\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [22:06<22:10, 266.20s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.0452, Validation Loss: 0.0598, accuracy = 0.6818, F1=0.6608\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [26:33<17:45, 266.41s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\nEpoch: 5, Training Loss: 0.0448, Validation Loss: 0.0607, accuracy = 0.6660, F1=0.6296\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [31:00<13:20, 266.71s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.0448, Validation Loss: 0.0526, accuracy = 0.7045, F1=0.6996\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [35:27<08:53, 266.72s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.0439, Validation Loss: 0.0523, accuracy = 0.7029, F1=0.6987\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [39:52<04:26, 266.15s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.0435, Validation Loss: 0.0527, accuracy = 0.7004, F1=0.6945\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [44:21<00:00, 266.13s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.0432, Validation Loss: 0.0538, accuracy = 0.7037, F1=0.6951\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, acc, f1 = evaluate(multimodal_transformer, test_dataloader, ce_loss)\nprint('Results on Test Set: ')\nprint(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:17:39.540745Z","iopub.execute_input":"2024-01-27T16:17:39.541111Z","iopub.status.idle":"2024-01-27T16:19:29.296694Z","shell.execute_reply.started":"2024-01-27T16:17:39.541084Z","shell.execute_reply":"2024-01-27T16:19:29.295616Z"},"trusted":true},"execution_count":186,"outputs":[{"name":"stdout","text":"Results on Test Set: \nTest loss: 0.055005217534751606\tAccuracy: 0.6858496953283684\tF1: 0.6796482801437378\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2 - Ensembling-Fusion","metadata":{}},{"cell_type":"markdown","source":"## Text-Only and Audio-Only Models ","metadata":{}},{"cell_type":"code","source":"class TextModel(nn.Module):\n    def __init__(self, tokenizer, embedder, head):\n        super().__init__()\n        #self.pos_encoder = PositionalEncoding(768, dual_modality=False)\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.head = head\n    def forward(self, texts, audio_features, audio_attention):\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['last_hidden_state']\n        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n        return self.head(text_features_pooled)\n    \nclass AudioModel(nn.Module):        \n    def __init__(self, transformer, head):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(768, dual_modality=False)\n        self.transformer = transformer\n        self.head = head\n        \n    def forward(self, texts, audio_features, audio_attention):\n        padding_mask = ~audio_attention.to(torch.bool)\n        audio_features = self.pos_encoder(audio_features)\n        full_attention_mask = torch.zeros((audio_features.shape[1],audio_features.shape[1]), dtype=torch.bool).to(device)\n        transformer_output = self.transformer(src=audio_features, mask=full_attention_mask, src_key_padding_mask=padding_mask)\n        \n        # pooling transformer output\n        transformer_output_sum = (transformer_output * audio_attention.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / audio_attention.sum(axis=1).unsqueeze(-1)\n        return self.head(transformer_output_pooled)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T17:26:05.668084Z","iopub.execute_input":"2024-01-26T17:26:05.668389Z","iopub.status.idle":"2024-01-26T17:26:05.682624Z","shell.execute_reply.started":"2024-01-26T17:26:05.668366Z","shell.execute_reply":"2024-01-26T17:26:05.681731Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling Model","metadata":{}},{"cell_type":"code","source":" class EnsemblingFusion(nn.Module):\n    def __init__(self, text_model, audio_model):\n        super().__init__()\n        self.text_model = text_model\n        self.audio_model = audio_model\n        self.weight = torch.nn.Parameter(torch.tensor(0.0))\n        \n    def forward(self, texts, audio_features, audio_attentions):\n        text_logits = self.text_model(texts, audio_features, audio_attentions)\n        audio_logits = self.audio_model(texts, audio_features, audio_attentions)\n        \n        text_probabilities = torch.nn.functional.softmax(text_logits)\n        audio_probabilities = torch.nn.functional.softmax(audio_logits)\n        \n        coefficient = (torch.tanh(self.weight) + 1) / 2\n        \n        coefficient = coefficient*0.4 + 0.3\n        \n        return coefficient*text_probabilities + (1-coefficient)*audio_probabilities","metadata":{"execution":{"iopub.status.busy":"2024-01-26T17:26:05.683683Z","iopub.execute_input":"2024-01-26T17:26:05.685063Z","iopub.status.idle":"2024-01-26T17:26:05.699669Z","shell.execute_reply.started":"2024-01-26T17:26:05.685037Z","shell.execute_reply":"2024-01-26T17:26:05.698866Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# TRAINING OF ENSEMBLING\nensembling_text_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\nensembling_audio_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\nensembling_transformer_layer = nn.TransformerEncoderLayer(d_model=768, nhead=4, dim_feedforward=512, batch_first=True).to(device)\nensembling_transformer_encoder = nn.TransformerEncoder(ensembling_transformer_layer, num_layers=4).to(device)\n\nensembling_text_model = TextModel(tokenizer, embedder, ensembling_text_head)\nensembling_audio_model = AudioModel(ensembling_transformer_encoder, ensembling_audio_head)\n\nensembling_fusion = EnsemblingFusion(ensembling_text_model, ensembling_audio_model).to(device)\n\nprint(f'#Params: {number_parameters(ensembling_fusion)}')\ndef custom_loss(outputs, targets):\n    return torch.nn.functional.nll_loss(torch.log(outputs), targets, reduction='mean')\n\ntrain(ensembling_fusion, custom_loss, train_dataloader, val_dataloader, epochs=10, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T17:28:15.659450Z","iopub.execute_input":"2024-01-26T17:28:15.659871Z","iopub.status.idle":"2024-01-26T17:51:03.358581Z","shell.execute_reply.started":"2024-01-26T17:28:15.659813Z","shell.execute_reply":"2024-01-26T17:51:03.357647Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"#Params: 13007879\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1/10 [02:16<20:29, 136.59s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.0773, Validation Loss: 0.0924, accuracy = 0.5789, F1=0.4541\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [04:33<18:12, 136.54s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.0737, Validation Loss: 0.0880, accuracy = 0.6043, F1=0.5121\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [06:49<15:55, 136.50s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.0726, Validation Loss: 0.0890, accuracy = 0.6047, F1=0.5120\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [09:06<13:39, 136.55s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.0721, Validation Loss: 0.0863, accuracy = 0.6145, F1=0.5347\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [11:22<11:23, 136.60s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.0715, Validation Loss: 0.0869, accuracy = 0.6164, F1=0.5395\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [13:39<09:06, 136.65s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.0713, Validation Loss: 0.0889, accuracy = 0.6064, F1=0.5174\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [15:56<06:50, 136.69s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.0709, Validation Loss: 0.0866, accuracy = 0.6166, F1=0.5400\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [18:13<04:33, 136.85s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\nEpoch: 7, Training Loss: 0.0706, Validation Loss: 0.0882, accuracy = 0.6095, F1=0.5221\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [20:30<02:16, 136.92s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.0713, Validation Loss: 0.0791, accuracy = 0.6570, F1=0.6196\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [22:47<00:00, 136.76s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.0705, Validation Loss: 0.0793, accuracy = 0.6551, F1=0.6157\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"train(ensembling_fusion, custom_loss, train_dataloader, val_dataloader, epochs=10, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T17:53:33.982206Z","iopub.execute_input":"2024-01-26T17:53:33.982885Z","iopub.status.idle":"2024-01-26T18:16:24.033480Z","shell.execute_reply.started":"2024-01-26T17:53:33.982848Z","shell.execute_reply":"2024-01-26T18:16:24.032556Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [02:17<20:33, 137.06s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.0705, Validation Loss: 0.0856, accuracy = 0.6214, F1=0.5508\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [04:33<18:15, 136.94s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.0705, Validation Loss: 0.0836, accuracy = 0.6331, F1=0.5715\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [06:50<15:58, 136.94s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.0706, Validation Loss: 0.0869, accuracy = 0.6160, F1=0.5365\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [09:07<13:41, 136.96s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.0704, Validation Loss: 0.0876, accuracy = 0.6097, F1=0.5251\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [11:24<11:24, 136.99s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.0706, Validation Loss: 0.0936, accuracy = 0.6022, F1=0.5065\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [13:42<09:08, 137.06s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\nEpoch: 5, Training Loss: 0.0705, Validation Loss: 0.0852, accuracy = 0.6226, F1=0.5523\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [15:59<06:51, 137.03s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.0709, Validation Loss: 0.0795, accuracy = 0.6560, F1=0.6185\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [18:16<04:34, 137.01s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.0707, Validation Loss: 0.0801, accuracy = 0.6518, F1=0.6089\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [20:33<02:17, 137.01s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.0705, Validation Loss: 0.0802, accuracy = 0.6505, F1=0.6058\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [22:50<00:00, 137.00s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.0704, Validation Loss: 0.0796, accuracy = 0.6539, F1=0.6132\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, acc, f1 = evaluate(ensembling_fusion, test_dataloader, ce_loss)\nprint('Results on Test Set: ')\nprint(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T18:16:24.035145Z","iopub.execute_input":"2024-01-26T18:16:24.035443Z","iopub.status.idle":"2024-01-26T18:17:05.502275Z","shell.execute_reply.started":"2024-01-26T18:16:24.035417Z","shell.execute_reply":"2024-01-26T18:17:05.501313Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Results on Test Set: \nTest loss: 0.11293967833935448\tAccuracy: 0.6420108327691266\tF1: 0.6087323427200317\n","output_type":"stream"}]},{"cell_type":"code","source":"ensembling_fusion.weight","metadata":{"execution":{"iopub.status.busy":"2024-01-26T18:17:37.121078Z","iopub.execute_input":"2024-01-26T18:17:37.121439Z","iopub.status.idle":"2024-01-26T18:17:37.197290Z","shell.execute_reply.started":"2024-01-26T18:17:37.121412Z","shell.execute_reply":"2024-01-26T18:17:37.196377Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor(0.8591, device='cuda:0', requires_grad=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3 - Text-Only","metadata":{}},{"cell_type":"markdown","source":"Text-only trained on the first two classes ignoring 'Other' class","metadata":{}},{"cell_type":"code","source":"text_only_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, MODEL_NUM_LABELS)\n).to(device)\n\ntext_only = TextModel(tokenizer, embedder, text_only_head)\n\ntrain(text_only, ce_loss, train_dataloader, val_dataloader, epochs=20, device=device)\n\ntest_loss, acc, f1 = evaluate(text_only, test_dataloader, ce_loss)\nprint('Results on Test Set: ')\nprint(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:18:47.192114Z","iopub.execute_input":"2024-01-26T15:18:47.193039Z","iopub.status.idle":"2024-01-26T15:44:46.654075Z","shell.execute_reply.started":"2024-01-26T15:18:47.193002Z","shell.execute_reply":"2024-01-26T15:44:46.653147Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"  5%|▌         | 1/20 [01:16<24:18, 76.75s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.1090, Validation Loss: 0.1326, accuracy = 0.5922, F1=0.4865\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [02:34<23:11, 77.30s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.1057, Validation Loss: 0.1317, accuracy = 0.5974, F1=0.4951\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [03:50<21:45, 76.77s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.1043, Validation Loss: 0.1297, accuracy = 0.6097, F1=0.5240\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [05:06<20:25, 76.56s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.1037, Validation Loss: 0.1295, accuracy = 0.6145, F1=0.5329\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [06:23<19:08, 76.55s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.1032, Validation Loss: 0.1324, accuracy = 0.6064, F1=0.5148\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [07:40<17:53, 76.67s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.1030, Validation Loss: 0.1283, accuracy = 0.6133, F1=0.5335\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 7/20 [08:56<16:34, 76.53s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.1022, Validation Loss: 0.1309, accuracy = 0.6141, F1=0.5306\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 8/20 [10:12<15:18, 76.51s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.1020, Validation Loss: 0.1297, accuracy = 0.6087, F1=0.5225\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 9/20 [11:28<13:59, 76.28s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.1019, Validation Loss: 0.1281, accuracy = 0.6208, F1=0.5459\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 10/20 [12:44<12:40, 76.05s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.1019, Validation Loss: 0.1325, accuracy = 0.6110, F1=0.5257\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 11/20 [13:59<11:22, 75.86s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Training Loss: 0.1014, Validation Loss: 0.1293, accuracy = 0.6197, F1=0.5407\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 12/20 [15:16<10:08, 76.10s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, Training Loss: 0.1016, Validation Loss: 0.1295, accuracy = 0.6176, F1=0.5400\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 13/20 [16:32<08:53, 76.17s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\nEpoch: 12, Training Loss: 0.1015, Validation Loss: 0.1289, accuracy = 0.6201, F1=0.5450\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 14/20 [17:49<07:37, 76.32s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Training Loss: 0.1011, Validation Loss: 0.1162, accuracy = 0.6464, F1=0.5952\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 15/20 [19:06<06:22, 76.51s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, Training Loss: 0.1009, Validation Loss: 0.1152, accuracy = 0.6455, F1=0.5953\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 16/20 [20:22<05:05, 76.31s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, Training Loss: 0.1007, Validation Loss: 0.1150, accuracy = 0.6464, F1=0.5970\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 17/20 [21:38<03:49, 76.38s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Training Loss: 0.1008, Validation Loss: 0.1167, accuracy = 0.6412, F1=0.5855\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 18/20 [22:55<02:32, 76.42s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Training Loss: 0.1001, Validation Loss: 0.1157, accuracy = 0.6447, F1=0.5916\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 19/20 [24:11<01:16, 76.43s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 18, Training Loss: 0.1007, Validation Loss: 0.1154, accuracy = 0.6445, F1=0.5931\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [25:28<00:00, 76.40s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 19, Training Loss: 0.1004, Validation Loss: 0.1144, accuracy = 0.6506, F1=0.6050\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Results on Test Set: \nTest loss: 0.11366133590871987\tAccuracy: 0.6352403520649966\tF1: 0.5967629551887512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4 - Unaligned Multimodal Model","metadata":{}},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n\nclass CrossModalAttentionBlock(nn.Module):\n    def __init__(self, embedding_dim, d_ffn):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.d_ffn = d_ffn\n        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n        self.mh_attention = nn.MultiheadAttention(self.embedding_dim, 4, 0.1, batch_first=True)\n        self.pointwise_ff = PositionwiseFeedForward(self.embedding_dim, d_ffn=self.d_ffn)\n    \n    def forward(self, elem_a, elem_b, attn_mask):\n        elem_a = self.layer_norm(elem_a)\n        elem_b = self.layer_norm(elem_b)\n        attn_mask = attn_mask.to(torch.float32)\n        \n        mh_out, _ = self.mh_attention(elem_a, elem_b, elem_b, key_padding_mask=attn_mask, need_weights=False)\n        add_out = mh_out + elem_a\n        \n        add_out_norm = self.layer_norm(add_out)\n        out_ffn = self.pointwise_ff(add_out_norm)\n        out = out_ffn + add_out\n        return out\n    \nclass UnalignedMultimodalModel(nn.Module):\n    def __init__(self, embedding_dim, d_ffn, n_blocks, head):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.d_ffn = d_ffn\n        self.n_blocks = n_blocks\n        self.head = head\n        self.text_crossmodal_blocks = nn.ModuleList([\n            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n        ])\n        self.audio_crossmodal_blocks = nn.ModuleList([\n            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n        ])\n        self.pos_encoder = PositionalEncoding(embedding_dim, dual_modality=False)\n    \n    def forward(self, texts, audio_features, audio_attentions):\n        tokenizer_output = tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][0]\n        text_features = self.pos_encoder(text_features)\n        text_attentions = tokenizer_output.attention_mask\n        \n        audio_features = self.pos_encoder(audio_features)\n        \n        text_crossmodal_out = text_features\n        for cm_block in self.text_crossmodal_blocks:\n            text_crossmodal_out = cm_block(text_crossmodal_out, audio_features, audio_attentions)\n        \n        audio_crossmodal_out = audio_features\n        for cm_block in self.audio_crossmodal_blocks:\n            audio_crossmodal_out = cm_block(audio_crossmodal_out, text_features, text_attentions)\n\n        text_crossmodal_out_mean = torch.mean(text_crossmodal_out, dim=1)\n        audio_crossmodal_out_mean = torch.mean(audio_crossmodal_out, dim=1)\n        \n        text_audio = torch.cat((text_crossmodal_out_mean, audio_crossmodal_out_mean), dim=-1)\n        \n        return self.head(text_audio)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T18:20:49.117899Z","iopub.execute_input":"2024-01-26T18:20:49.118507Z","iopub.status.idle":"2024-01-26T18:20:49.134944Z","shell.execute_reply.started":"2024-01-26T18:20:49.118472Z","shell.execute_reply":"2024-01-26T18:20:49.134006Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# TRAINING OF UNALIGNED-MODEL\nunaligned_head = nn.Sequential(\n    nn.Linear(768*2, 256),\n    nn.ReLU(),\n    nn.Linear(256, MODEL_NUM_LABELS)\n).to(device)\n\nunaligned_mm_model = UnalignedMultimodalModel(768, 100, 4, unaligned_head).to(device)\n\ntrain(unaligned_mm_model, ce_loss, train_dataloader, val_dataloader, epochs=20, device=device)\n\ntest_loss, acc, f1 = evaluate(unaligned_mm_model, test_dataloader, ce_loss)\nprint('Results on Test Set: ')\nprint(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T18:21:20.675779Z","iopub.execute_input":"2024-01-26T18:21:20.676638Z","iopub.status.idle":"2024-01-26T19:04:17.823914Z","shell.execute_reply.started":"2024-01-26T18:21:20.676597Z","shell.execute_reply":"2024-01-26T19:04:17.822964Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"  5%|▌         | 1/20 [02:05<39:49, 125.74s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.0864, Validation Loss: 0.0948, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 2/20 [04:11<37:48, 126.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.0852, Validation Loss: 0.0904, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 3/20 [06:18<35:45, 126.19s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.0851, Validation Loss: 0.0890, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 4/20 [08:24<33:40, 126.30s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.0852, Validation Loss: 0.0896, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 5/20 [10:31<31:39, 126.61s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.0851, Validation Loss: 0.0910, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 6/20 [12:38<29:33, 126.65s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.0848, Validation Loss: 0.0957, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 7/20 [14:45<27:26, 126.63s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch: 6, Training Loss: 0.0849, Validation Loss: 0.0928, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 8/20 [16:52<25:20, 126.72s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.0866, Validation Loss: 0.0863, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 9/20 [19:00<23:18, 127.10s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.0864, Validation Loss: 0.0863, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 10/20 [21:07<21:11, 127.17s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.0863, Validation Loss: 0.0865, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 11/20 [23:13<19:02, 126.94s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Training Loss: 0.0861, Validation Loss: 0.0866, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 12/20 [25:21<16:56, 127.07s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\nEpoch: 11, Training Loss: 0.0860, Validation Loss: 0.0868, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 13/20 [27:28<14:50, 127.15s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 14/20 [29:36<12:43, 127.23s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 15/20 [31:43<10:36, 127.26s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 16/20 [33:50<08:29, 127.31s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\nEpoch: 15, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 17/20 [35:58<06:21, 127.31s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 18/20 [38:04<04:14, 127.07s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 19/20 [40:11<02:06, 126.95s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 18, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [42:17<00:00, 126.89s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\nEpoch: 19, Training Loss: 0.0864, Validation Loss: 0.0864, accuracy = 0.5414, F1=0.3513\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Results on Test Set: \nTest loss: 0.08669511902227382\tAccuracy: 0.5140487474610698\tF1: 0.3395192623138428\n","output_type":"stream"}]}]}