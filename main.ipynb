{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7402218,"sourceType":"datasetVersion","datasetId":4304373},{"sourceId":7480566,"sourceType":"datasetVersion","datasetId":4354582},{"sourceId":160785062,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"!pip install torcheval","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:37:41.688720Z","iopub.execute_input":"2024-01-28T21:37:41.689127Z","iopub.status.idle":"2024-01-28T21:37:57.272049Z","shell.execute_reply.started":"2024-01-28T21:37:41.689094Z","shell.execute_reply":"2024-01-28T21:37:57.270758Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torcheval\n  Obtaining dependency information for torcheval from https://files.pythonhosted.org/packages/e4/de/e7abc784b00de9d05999657d29187f1f7a3406ed10ecaf164de06482608f/torcheval-0.0.7-py3-none-any.whl.metadata\n  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.5.0)\nDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch, torchaudio, torchtext\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport warnings\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom torcheval.metrics.functional import multiclass_f1_score\nfrom transformers import BertTokenizer, BertModel, AutoModel, AutoProcessor\nfrom tqdm import tqdm\n\ntry:\n    from CustomTransformer import CustomEncoder, PositionalEncoding, LayerNorm\nexcept:\n    from customtransformer import CustomEncoder, PositionalEncoding, LayerNorm\n    \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T23:39:44.460063Z","iopub.execute_input":"2024-01-28T23:39:44.460938Z","iopub.status.idle":"2024-01-28T23:39:44.469649Z","shell.execute_reply.started":"2024-01-28T23:39:44.460890Z","shell.execute_reply":"2024-01-28T23:39:44.468688Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"### Constants","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')\n\nMODEL_NUM_LABELS = 3\nREMOVE_OTHER = True\nOTHER_LABEL = 'O'\n    \nif REMOVE_OTHER:\n    MODEL_NUM_LABELS = 2\n\nEMBEDDING_DIM = 768\nBATCH_SIZE = 12","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:16.504110Z","iopub.execute_input":"2024-01-28T21:38:16.504711Z","iopub.status.idle":"2024-01-28T21:38:16.511637Z","shell.execute_reply.started":"2024-01-28T21:38:16.504683Z","shell.execute_reply":"2024-01-28T21:38:16.510501Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load df","metadata":{}},{"cell_type":"code","source":"try:\n    # Try to load from Kaggle\n    df_path = '/kaggle/input/multimodal-argument-mining/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = '/kaggle/input/multimodal-argument-mining/MM-USElecDeb60to16/audio_clips'\n    save_path = '/kaggle/input/mm-dataset-subsampling/'\n    df = pd.read_csv(df_path, index_col=0)\nexcept FileNotFoundError:\n    # Try to load from local\n    df_path = 'multimodal-dataset/files/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = 'multimodal-dataset/files/MM-USElecDeb60to16/audio_clips'\n    save_path = 'multimodal-dataset/files'\n    df = pd.read_csv(df_path, index_col=0)\n    \n# drop rows where audio length is 0\ndf = df[df['NewBegin'] != df['NewEnd']]\nif REMOVE_OTHER:\n    # drop rows where Component is 'Other'\n    df = df[df['Component'] != OTHER_LABEL]\n\n# train, val, test split\ntrain_df_complete = df[df['Set'] == 'TRAIN']\nval_df_complete = df[df['Set'] == 'VALIDATION']\ntest_df_complete = df[df['Set'] == 'TEST']\n\n# subsample datasets for memory reasons\nDATASET_RATIO = 1\ntrain_df = train_df_complete.iloc[:int(DATASET_RATIO * len(train_df_complete))]\nval_df = val_df_complete.iloc[:int(DATASET_RATIO * len(val_df_complete))]\ntest_df = test_df_complete.iloc[:int(DATASET_RATIO * len(test_df_complete))]","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:16.515139Z","iopub.execute_input":"2024-01-28T21:38:16.515593Z","iopub.status.idle":"2024-01-28T21:38:16.894386Z","shell.execute_reply.started":"2024-01-28T21:38:16.515553Z","shell.execute_reply":"2024-01-28T21:38:16.893182Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:16.895779Z","iopub.execute_input":"2024-01-28T21:38:16.896206Z","iopub.status.idle":"2024-01-28T21:38:16.926697Z","shell.execute_reply.started":"2024-01-28T21:38:16.896168Z","shell.execute_reply":"2024-01-28T21:38:16.925705Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                 Text  Part Document  Order  \\\n3   And, after 9/11, it became clear that we had t...     1  30_2004      3   \n4   And we also then finally had to stand up democ...     1  30_2004      4   \n9   What we did in Iraq was exactly the right thin...     1  30_2004      9   \n10  If I had it to recommend all over again, I wou...     1  30_2004     10   \n11  The world is far safer today because Saddam Hu...     1  30_2004     11   \n\n    Sentence  Start   End  Annotator                                   Tag  \\\n3          3   2418  2744        NaN                {\"O\": 16, \"Claim\": 50}   \n4          4   2744  2974        NaN  {\"O\": 4, \"Claim\": 13, \"Premise\": 25}   \n9          9   3861  3916        NaN                 {\"Claim\": 12, \"O\": 1}   \n10        10   3916  4010        NaN               {\"Premise\": 19, \"O\": 1}   \n11        11   4010  4112        NaN   {\"Claim\": 6, \"O\": 2, \"Premise\": 13}   \n\n   Component  ... Speaker SpeakerType    Set         Date  Year  \\\n3      Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n4    Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n9      Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n10   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n11   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n\n                       Name  MainTag NewBegin  NewEnd   idClip  \n3   Richard(Dick) B. Cheney    Claim   140.56  158.92   clip_3  \n4   Richard(Dick) B. Cheney    Mixed   158.92  172.92   clip_4  \n9   Richard(Dick) B. Cheney    Claim   224.08  226.88   clip_9  \n10  Richard(Dick) B. Cheney  Premise   226.88  231.56  clip_10  \n11  Richard(Dick) B. Cheney    Mixed   231.56  237.56  clip_11  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Part</th>\n      <th>Document</th>\n      <th>Order</th>\n      <th>Sentence</th>\n      <th>Start</th>\n      <th>End</th>\n      <th>Annotator</th>\n      <th>Tag</th>\n      <th>Component</th>\n      <th>...</th>\n      <th>Speaker</th>\n      <th>SpeakerType</th>\n      <th>Set</th>\n      <th>Date</th>\n      <th>Year</th>\n      <th>Name</th>\n      <th>MainTag</th>\n      <th>NewBegin</th>\n      <th>NewEnd</th>\n      <th>idClip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>And, after 9/11, it became clear that we had t...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2418</td>\n      <td>2744</td>\n      <td>NaN</td>\n      <td>{\"O\": 16, \"Claim\": 50}</td>\n      <td>Claim</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Claim</td>\n      <td>140.56</td>\n      <td>158.92</td>\n      <td>clip_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And we also then finally had to stand up democ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2744</td>\n      <td>2974</td>\n      <td>NaN</td>\n      <td>{\"O\": 4, \"Claim\": 13, \"Premise\": 25}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Mixed</td>\n      <td>158.92</td>\n      <td>172.92</td>\n      <td>clip_4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>What we did in Iraq was exactly the right thin...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>9</td>\n      <td>9</td>\n      <td>3861</td>\n      <td>3916</td>\n      <td>NaN</td>\n      <td>{\"Claim\": 12, \"O\": 1}</td>\n      <td>Claim</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Claim</td>\n      <td>224.08</td>\n      <td>226.88</td>\n      <td>clip_9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>If I had it to recommend all over again, I wou...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>10</td>\n      <td>10</td>\n      <td>3916</td>\n      <td>4010</td>\n      <td>NaN</td>\n      <td>{\"Premise\": 19, \"O\": 1}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Premise</td>\n      <td>226.88</td>\n      <td>231.56</td>\n      <td>clip_10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The world is far safer today because Saddam Hu...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>11</td>\n      <td>11</td>\n      <td>4010</td>\n      <td>4112</td>\n      <td>NaN</td>\n      <td>{\"Claim\": 6, \"O\": 2, \"Premise\": 13}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Mixed</td>\n      <td>231.56</td>\n      <td>237.56</td>\n      <td>clip_11</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(train_df), len(test_df), len(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:16.927974Z","iopub.execute_input":"2024-01-28T21:38:16.928282Z","iopub.status.idle":"2024-01-28T21:38:16.935466Z","shell.execute_reply.started":"2024-01-28T21:38:16.928254Z","shell.execute_reply":"2024-01-28T21:38:16.934318Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(9455, 5908, 5201)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Distribution of classes over train df","metadata":{}},{"cell_type":"code","source":"num_claim = len(train_df[train_df['Component'] == 'Claim'])\nprint(f'Total Claim: {num_claim}: {num_claim*100/len(train_df):.2f}%')\n\nnum_premise = len(train_df[train_df['Component'] == 'Premise'])\nprint(f'Total Premise: {num_premise}: {num_premise*100/len(train_df):.2f}%')\n\nif not REMOVE_OTHER:\n    num_other = len(train_df[train_df['Component'] == 'O'])\n    print(f'Total Other: {num_other}: {num_other*100/len(train_df):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:16.937151Z","iopub.execute_input":"2024-01-28T21:38:16.937753Z","iopub.status.idle":"2024-01-28T21:38:16.954782Z","shell.execute_reply.started":"2024-01-28T21:38:16.937711Z","shell.execute_reply":"2024-01-28T21:38:16.953713Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Total Claim: 5029: 53.19%\nTotal Premise: 4426: 46.81%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Classes are not balanced, but not too bad either.","metadata":{}},{"cell_type":"markdown","source":"# Train and evaluation Loop","metadata":{}},{"cell_type":"code","source":"ce_loss = nn.CrossEntropyLoss()\n\nclass BestModel:\n    \"\"\"\n    Class to keep track of the best performing model on validation set during training\n    \"\"\"\n    def __init__(self):\n        self.best_validation_loss = float('Infinity')\n        self.best_state_dict = None\n    def __call__(self, model, loss):\n        if loss < self.best_validation_loss:\n            self.best_validation_loss = loss\n            self.best_state_dict = model.state_dict()\n\ndef evaluate(model, data_loader, loss_fn, debug=False):\n    \"\"\"\n    Evaluate the model on the set passed\n    Args:\n        model: model to evaluate\n        data_loader: DataLoader object\n        loss_fn: loss function to use\n    \"\"\"\n    model.eval()\n    valid_loss = 0.0\n    num_correct = 0 \n    num_examples = 0\n    tot_pred, tot_targ = torch.LongTensor().to(device), torch.LongTensor().to(device)\n    for batch in data_loader:\n        texts, audio_features, audio_attention, targets = batch\n        audio_features = audio_features.to(device)\n        audio_attention = audio_attention.to(device)\n        targets = targets.to(device)\n        output = model(texts,audio_features,audio_attention)\n        if debug:\n            print(\"OUTPUT\",output)\n            print(\"TARGETS\", targets)\n        loss = loss_fn(output, targets)\n        valid_loss += loss.detach()\n        \n        # if label O is still in the dataset we remove it from the outputs\n        # since it's a binary task\n        if not REMOVE_OTHER:\n            not_other = targets != 2\n            output = output[not_other]\n            targets = targets[not_other]\n        \n        predicted_labels = torch.argmax(output[:, :2], dim=-1)\n        tot_targ = torch.cat((tot_targ, targets))\n        tot_pred = torch.cat((tot_pred, predicted_labels))            \n        correct = torch.eq(predicted_labels, targets).view(-1)\n        num_correct += torch.sum(correct).item()\n        num_examples += correct.shape[0]\n    valid_loss = valid_loss.cpu().item()\n    valid_loss /= len(data_loader.dataset)\n    accuracy = num_correct/num_examples\n    f1 = multiclass_f1_score(tot_pred, tot_targ, num_classes=2, average=\"macro\")\n    return valid_loss, accuracy, f1, tot_pred, tot_targ\n\n            \ndef train(model, loss_fn, train_loader, val_loader, epochs=10, device=\"cuda\", lr=1e-3, lr_decay_factor=0.1, lr_decay_patience=3, weight_decay=1e-5, verbose=True, debug=False):\n    \"\"\"\n    Train the model on the train set and evaluate on the validation set with the given parameters\n    Args:\n        model: model to train\n        loss_fn: loss function to use\n        train_loader: DataLoader object for train set\n        val_loader: DataLoader object for validation set\n        epochs: number of epochs\n        device: device to use\n        lr: initial learning rate\n        lr_decay_factor: factor to decay learning rate\n        lr_decay_patience: patience for learning rate decay\n        weight_decay: weight decay\n    \"\"\"\n    # set up optimizer and scheduler\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_decay_patience, verbose=True)\n    best_model_tracker = BestModel()\n    for epoch in tqdm(range(epochs)):\n        training_loss = 0.0\n        model.train()\n        for batch in train_loader:\n            optimizer.zero_grad()\n            texts, audio_features, audio_attention, targets = batch\n            audio_features = audio_features.to(device)\n            audio_attention = audio_attention.to(device)\n            targets = targets.to(device)\n            output = model(texts,audio_features,audio_attention)\n            loss = loss_fn(output, targets)\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.detach()\n        training_loss = training_loss.cpu().item()\n        training_loss /= len(train_loader.dataset)\n        valid_loss, accuracy, f1, _, _ = evaluate(model, val_loader, loss_fn, debug)\n        best_model_tracker(model, valid_loss)\n        scheduler.step(valid_loss)\n        if verbose:\n            print(f'Epoch: {epoch}, Training Loss: {training_loss:.4f}, Validation Loss: {valid_loss:.4f}, accuracy = {accuracy:.4f}, F1={f1:.4f}')\n    model.load_state_dict(best_model_tracker.best_state_dict)    ","metadata":{"execution":{"iopub.status.busy":"2024-01-28T22:10:28.088061Z","iopub.execute_input":"2024-01-28T22:10:28.088855Z","iopub.status.idle":"2024-01-28T22:10:28.112084Z","shell.execute_reply.started":"2024-01-28T22:10:28.088812Z","shell.execute_reply":"2024-01-28T22:10:28.111014Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Creation","metadata":{}},{"cell_type":"code","source":"# set up tokenizer and model\ntext_model_card = 'bert-base-uncased'\naudio_model_card = 'facebook/wav2vec2-base-960h'\n\ntokenizer = BertTokenizer.from_pretrained(text_model_card)\nembedder = BertModel.from_pretrained(text_model_card).to(device)\n\n# freeze bert layers\nfor params in embedder.parameters():\n    params.requires_grad = False\n\nlabel_2_id = {\n    'Claim': 0,\n    'Premise': 1,\n    'O': 2\n}\n\n# Downsample audio features to 1/5 of the original size to fit in memory\nDOWNSAMPLE_FACTOR = 1/5\n\nclass MM_Dataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for multimodal dataset\n    \"\"\"\n    def __init__(self, df, audio_dir, sample_rate):\n        \"\"\"\n        Args:\n            df: dataframe containing the dataset\n            audio_dir: directory containing the audio clips\n            sample_rate: sample rate to use for audio clips\n        \"\"\"\n        self.audio_dir = audio_dir\n        self.sample_rate = sample_rate\n\n        self.audio_processor = AutoProcessor.from_pretrained(audio_model_card)\n        self.audio_model = AutoModel.from_pretrained(audio_model_card).to(device)\n\n        self.dataset = []\n\n        # Iterate over df\n        for _, row in tqdm(df.iterrows()):\n            path = os.path.join(self.audio_dir, f\"{row['Document']}/{row['idClip']}.wav\")\n            if os.path.exists(path):\n                # obtain audio WAV2VEC features\n                audio, sampling_rate = torchaudio.load(path)\n                # resample audio if necessary\n                if sampling_rate != self.sample_rate:\n                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\n                    # mean pooling over channels\n                    audio = torch.mean(audio, dim=0, keepdim=True)\n                with torch.inference_mode():\n                    # run audio through model\n                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\n                    input_values = torch.tensor(input_values).to(device)\n                    audio_model_output = self.audio_model(input_values)\n                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\n                    # downsample audio features\n                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode='linear')\n                    audio_features = audio_features.permute(0,2,1)[0]\n                    audio_features = audio_features.cpu()\n                \n                text = row['Text']\n\n                self.dataset.append((text, audio_features, label_2_id[row['Component']]))\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        return self.dataset[index]","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:16.982328Z","iopub.execute_input":"2024-01-28T21:38:16.982952Z","iopub.status.idle":"2024-01-28T21:38:20.753074Z","shell.execute_reply.started":"2024-01-28T21:38:16.982911Z","shell.execute_reply":"2024-01-28T21:38:20.752050Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1ba8f9f8b74c0da48bcc2eaf8a68a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358fc34c055146ff9d145fbbba8d59b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7c0b0c976644979ecd58e1156a925e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eab25122ade48a9ab554dd6fa70dba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae2abe83c9143839c069127391621e7"}},"metadata":{}}]},{"cell_type":"code","source":"try:\n    train_dataset = torch.load(f'{save_path}/train_dataset.pkl')\n    test_dataset = torch.load(f'{save_path}/test_dataset.pkl')\n    val_dataset = torch.load(f'{save_path}/val_dataset.pkl')\n    if REMOVE_OTHER:\n        train_dataset = list(filter(lambda x: x[2] != 2, train_dataset))\n        test_dataset = list(filter(lambda x: x[2] != 2, test_dataset))\n        val_dataset = list(filter(lambda x: x[2] != 2, val_dataset))\n    print('Restored datasets from memory')\nexcept:\n    print('Creating new datasets')\n    train_dataset = MM_Dataset(train_df, audio_path, 16_000)\n    test_dataset = MM_Dataset(test_df, audio_path, 16_000)\n    val_dataset = MM_Dataset(val_df, audio_path, 16_000)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:38:20.757442Z","iopub.execute_input":"2024-01-28T21:38:20.757774Z","iopub.status.idle":"2024-01-28T21:40:36.900393Z","shell.execute_reply.started":"2024-01-28T21:38:20.757746Z","shell.execute_reply":"2024-01-28T21:40:36.899346Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Restored datasets from memory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataloader creation","metadata":{}},{"cell_type":"code","source":"def create_dataloader(dataset, batch_size):\n    \"\"\"\n    Create a DataLoader object from the given dataset with the given batch size\n    Args:\n        dataset: dataset to use\n        batch_size: batch size to use\n    \"\"\"\n    def pack_fn(batch):\n        \"\"\"\n        Function to pad the audio features and create the attention mask\n        \"\"\"\n        texts = [x[0] for x in batch]\n        audio_features = [x[1] for x in batch]\n        labels = torch.tensor([x[2] for x in batch])\n        \n        # pad audio features\n        audio_features = pad_sequence(audio_features, batch_first=True, padding_value=float('-inf'))\n        audio_features_attention_mask = audio_features[:, :, 0] != float('-inf')\n        audio_features[(audio_features == float('-inf'))] = 0\n        return texts, audio_features, audio_features_attention_mask, labels\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pack_fn)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:36.911022Z","iopub.execute_input":"2024-01-28T21:40:36.911374Z","iopub.status.idle":"2024-01-28T21:40:36.919961Z","shell.execute_reply.started":"2024-01-28T21:40:36.911344Z","shell.execute_reply":"2024-01-28T21:40:36.918734Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataloader = create_dataloader(train_dataset, BATCH_SIZE)\nval_dataloader = create_dataloader(val_dataset, BATCH_SIZE)\ntest_dataloader = create_dataloader(test_dataset, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:36.921746Z","iopub.execute_input":"2024-01-28T21:40:36.922208Z","iopub.status.idle":"2024-01-28T21:40:36.939503Z","shell.execute_reply.started":"2024-01-28T21:40:36.922171Z","shell.execute_reply":"2024-01-28T21:40:36.938657Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:36.940772Z","iopub.execute_input":"2024-01-28T21:40:36.941169Z","iopub.status.idle":"2024-01-28T21:40:37.284480Z","shell.execute_reply.started":"2024-01-28T21:40:36.941142Z","shell.execute_reply":"2024-01-28T21:40:37.283512Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"18"},"metadata":{}}]},{"cell_type":"code","source":"def number_parameters(model):\n    \"\"\"\n    Computes the number of trainable parameters in the model\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:37.286088Z","iopub.execute_input":"2024-01-28T21:40:37.286521Z","iopub.status.idle":"2024-01-28T21:40:37.297927Z","shell.execute_reply.started":"2024-01-28T21:40:37.286475Z","shell.execute_reply":"2024-01-28T21:40:37.296929Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 0-A Text-Only","metadata":{}},{"cell_type":"code","source":"class TextModel(nn.Module):\n    \"\"\"\n    Class for the text-only model\n    \"\"\"\n    def __init__(self, tokenizer, embedder, head):\n        \"\"\"\n        Args:\n            tokenizer: tokenizer to use\n            embedder: embedder to use\n            head: head to use\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.head = head\n    def forward(self, texts, audio_features, audio_attention):\n        \"\"\"\n        Forward pass of the model\n        Args:\n            texts: texts to use\n            audio_features: audio features to use\n            audio_attentions: audio attentions to use\n        \"\"\"\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['last_hidden_state']\n\n        # pooling transformer output\n        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n        return self.head(text_features_pooled)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:37.299060Z","iopub.execute_input":"2024-01-28T21:40:37.299321Z","iopub.status.idle":"2024-01-28T21:40:37.310905Z","shell.execute_reply.started":"2024-01-28T21:40:37.299297Z","shell.execute_reply":"2024-01-28T21:40:37.309938Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# 0-B Audio-Only","metadata":{}},{"cell_type":"code","source":"class AudioModel(nn.Module):        \n    \"\"\"\n    Class for the audio-only model\n    \"\"\"\n    def __init__(self, transformer, head):\n        \"\"\"\n        Args:\n            transformer: transformer to use\n            head: head to use\n        \"\"\"\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(EMBEDDING_DIM, dual_modality=False)\n        self.transformer = transformer\n        self.head = head\n        self.ln = LayerNorm(EMBEDDING_DIM)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, texts, audio_features, audio_attention):\n        global hard_debug\n        \"\"\"\n        Forward pass of the model\n        Args:\n            texts: texts to use\n            audio_features: audio features to use\n            audio_attentions: audio attentions to use\n        \"\"\"\n        padding_mask = ~audio_attention.to(torch.bool)        \n        full_attention_mask = torch.zeros((audio_features.shape[1],audio_features.shape[1]), dtype=torch.bool).to(device)\n        \n        audio_features = self.pos_encoder(audio_features)\n        \n        transformer_output = self.transformer(audio_features, mask=full_attention_mask, src_key_padding_mask=padding_mask)\n        \n        # Dropout and LayerNorm to help training phase\n        transformer_output = self.dropout(transformer_output)\n        transformer_output = self.ln(audio_features + transformer_output)\n\n        transformer_output_sum = (transformer_output * audio_attention.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / audio_attention.sum(axis=1).unsqueeze(-1)\n\n        return self.head(transformer_output_pooled)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T23:49:46.672457Z","iopub.execute_input":"2024-01-28T23:49:46.673116Z","iopub.status.idle":"2024-01-28T23:49:46.683291Z","shell.execute_reply.started":"2024-01-28T23:49:46.673081Z","shell.execute_reply":"2024-01-28T23:49:46.682259Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Multimodal-Transformer","metadata":{}},{"cell_type":"code","source":"class MultiModalTransformer(nn.Module):\n    \"\"\"\n    Class for the multimodal transformer model\n    \"\"\"\n    def __init__(self, tokenizer, embedder, transformer, head):\n        \"\"\"\n        Args:\n            tokenizer: tokenizer to use\n            embedder: embedder to use\n            transformer: transformer to use\n            head: head to use\n        \"\"\"\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(EMBEDDING_DIM, dual_modality=False)\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.transformer = transformer\n        self.head = head\n\n    def forward(self, texts, audio_features, audio_attentions):\n        \"\"\"\n        Forward pass of the model\n        Args:\n            texts: texts to use\n            audio_features: audio features to use\n            audio_attentions: audio attentions to use\n        \"\"\"\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][0]\n        text_attentions = tokenizer_output.attention_mask\n\n        concatenated_attentions = torch.cat((text_attentions, audio_attentions.float()), dim=1)\n        \n        audio_features = self.pos_encoder(audio_features)\n        \n        concatenated_features = torch.cat((text_features, audio_features), dim=1)\n\n        transformer_output = self.transformer(concatenated_features, text_attentions, audio_attentions)\n\n        # pooling of transformer output        \n        transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1)\n        return self.head(transformer_output_pooled)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:37.329200Z","iopub.execute_input":"2024-01-28T21:40:37.329647Z","iopub.status.idle":"2024-01-28T21:40:37.343249Z","shell.execute_reply.started":"2024-01-28T21:40:37.329612Z","shell.execute_reply":"2024-01-28T21:40:37.342224Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Ensembling-Fusion","metadata":{}},{"cell_type":"code","source":" class EnsemblingFusion(nn.Module):\n    \"\"\"\n    Class for the ensembling model\n    \"\"\"\n    def __init__(self, text_model, audio_model):\n        \"\"\"\n        Args:\n            text_model: text model to use\n            audio_model: audio model to use\n        \"\"\"\n        super().__init__()\n        self.text_model = text_model\n        self.audio_model = audio_model\n        # weight to balance the two models\n        self.weight = torch.nn.Parameter(torch.tensor(0.0))\n        \n    def forward(self, texts, audio_features, audio_attentions):\n        \"\"\"\n        Forward pass of the model\n        Args:\n            texts: texts to use\n            audio_features: audio features to use\n            audio_attentions: audio attentions to use\n        \"\"\"\n        text_logits = self.text_model(texts, audio_features, audio_attentions)\n        audio_logits = self.audio_model(texts, audio_features, audio_attentions)\n        \n        text_probabilities = torch.nn.functional.softmax(text_logits)\n        audio_probabilities = torch.nn.functional.softmax(audio_logits)\n        \n        # coefficient to balance the two models based on weight learned\n        # (tanh + 1) / 2 to have values in [0,1]\n        coefficient = (torch.tanh(self.weight) + 1) / 2\n        # next step is to have values in [0.3,0.7] to avoid too much imbalance\n        coefficient = coefficient*0.4 + 0.3\n        \n        return coefficient*text_probabilities + (1-coefficient)*audio_probabilities","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:37.344527Z","iopub.execute_input":"2024-01-28T21:40:37.344855Z","iopub.status.idle":"2024-01-28T21:40:37.359781Z","shell.execute_reply.started":"2024-01-28T21:40:37.344828Z","shell.execute_reply":"2024-01-28T21:40:37.358897Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Unaligned Multimodal Model","metadata":{}},{"cell_type":"code","source":"class UnalignedPositionwiseFeedForward(nn.Module):\n    \"\"\"\n    Class for the positionwise feed forward layer\n    \"\"\"\n    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n        \"\"\"\n        Args:\n            d_model: dimension of the model\n            d_ffn: dimension of the feed forward layer\n            dropout: dropout to use\n        \"\"\"\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model\n        Args:\n            x: input to use\n        \"\"\"\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n\nclass CrossModalAttentionBlock(nn.Module):\n    \"\"\"\n    Class for the cross modal attention block\n    \"\"\"\n    def __init__(self, embedding_dim, d_ffn):\n        \"\"\"\n        Args:\n            embedding_dim: dimension of the embedding\n            d_ffn: dimension of the feed forward layer\n        \"\"\"\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.d_ffn = d_ffn\n        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n        self.mh_attention = nn.MultiheadAttention(self.embedding_dim, 4, 0.1, batch_first=True)\n        self.pointwise_ff = UnalignedPositionwiseFeedForward(self.embedding_dim, d_ffn=self.d_ffn)\n    \n    def forward(self, elem_a, elem_b, attn_mask):\n        \"\"\"\n        Forward pass of the model\n        Args:\n            elem_a: elements of the modality A\n            elem_b: elements of the modality B\n            attn_mask: attention mask to use\n        \"\"\"\n        elem_a = self.layer_norm(elem_a)\n        elem_b = self.layer_norm(elem_b)\n        attn_mask = attn_mask.to(torch.float32)\n        \n        # cross modal attention with elem_a as query and elem_b as key and value\n        mh_out, _ = self.mh_attention(elem_a, elem_b, elem_b, key_padding_mask=attn_mask, need_weights=False)\n        # residual connection\n        add_out = mh_out + elem_a\n        \n        add_out_norm = self.layer_norm(add_out)\n        out_ffn = self.pointwise_ff(add_out_norm)\n        out = out_ffn + add_out\n        return out\n    \nclass UnalignedMultimodalModel(nn.Module):\n    \"\"\"\n    Class for the unaligned multimodal model\n    \"\"\"\n    def __init__(self, embedding_dim, d_ffn, n_blocks, head):\n        \"\"\"\n        Args:\n            embedding_dim: dimension of the embedding\n            d_ffn: dimension of the feed forward layer\n            n_blocks: number of blocks to use\n            head: head to use\n        \"\"\"\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.d_ffn = d_ffn\n        self.n_blocks = n_blocks\n        self.head = head\n        self.text_crossmodal_blocks = nn.ModuleList([\n            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n        ])\n        self.audio_crossmodal_blocks = nn.ModuleList([\n            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n        ])\n        self.pos_encoder = PositionalEncoding(embedding_dim, dual_modality=False)\n    \n    def forward(self, texts, audio_features, audio_attentions):\n        \"\"\"\n        Forward pass of the model\n        Args:\n            texts: texts to use\n            audio_features: audio features to use\n            audio_attentions: audio attentions to use\n        \"\"\"\n        tokenizer_output = tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][0]\n        text_features = self.pos_encoder(text_features)\n        text_attentions = tokenizer_output.attention_mask\n        \n        audio_features = self.pos_encoder(audio_features)\n        \n        # cross modal attention blocks for text\n        # using audio features as key and value and text features as query\n        text_crossmodal_out = text_features\n        for cm_block in self.text_crossmodal_blocks:\n            text_crossmodal_out = cm_block(text_crossmodal_out, audio_features, audio_attentions)\n        \n        # cross modal attention blocks for audio\n        # using text features as key and value and audio features as query\n        audio_crossmodal_out = audio_features\n        for cm_block in self.audio_crossmodal_blocks:\n            audio_crossmodal_out = cm_block(audio_crossmodal_out, text_features, text_attentions)\n\n        # pooling of transformer output\n        text_crossmodal_out_mean = torch.mean(text_crossmodal_out, dim=1)\n        audio_crossmodal_out_mean = torch.mean(audio_crossmodal_out, dim=1)\n        \n        # concatenate text and audio features\n        text_audio = torch.cat((text_crossmodal_out_mean, audio_crossmodal_out_mean), dim=-1)\n        \n        return self.head(text_audio)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:40:37.361085Z","iopub.execute_input":"2024-01-28T21:40:37.361400Z","iopub.status.idle":"2024-01-28T21:40:37.382781Z","shell.execute_reply.started":"2024-01-28T21:40:37.361374Z","shell.execute_reply":"2024-01-28T21:40:37.381933Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Training of the models","metadata":{}},{"cell_type":"code","source":"def create_models():\n    \"\"\"\n    Creates all the models\n    \"\"\"\n    ###################################################################################### -- TEXT MODEL --\n\n    text_only_head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM, 256),\n        nn.ReLU(),\n        nn.Linear(256, MODEL_NUM_LABELS)\n    ).to(device)\n    text_only = TextModel(tokenizer, embedder, text_only_head)\n\n    ###################################################################################### -- AUDIO MODEL --\n    \n    audio_only_head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM, 256),\n        nn.ReLU(),\n        nn.Linear(256, MODEL_NUM_LABELS)\n    ).to(device)\n    audio_only_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=8, dim_feedforward=100, batch_first=True).to(device)\n    audio_only_transformer_encoder = nn.TransformerEncoder(audio_only_transformer_layer, num_layers=1).to(device)\n    audio_only = AudioModel(audio_only_transformer_encoder, audio_only_head).to(device)\n\n    ###################################################################################### -- MULTIMODAL MODEL --\n    \n    multimodal_encoder = CustomEncoder(d_model=EMBEDDING_DIM, ffn_hidden=2048, n_head=4, n_layers=1, drop_prob=0.1)\n    multimodal_transformer_head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM, 256),\n        nn.ReLU(),\n        nn.Linear(256, MODEL_NUM_LABELS)\n    ).to(device)\n    multimodal_transformer = MultiModalTransformer(tokenizer, embedder, multimodal_encoder, multimodal_transformer_head).to(device)\n\n    ###################################################################################### -- ENSEMBLING MODEL --\n\n    ensembling_text_head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM, 256),\n        nn.ReLU(),\n        nn.Linear(256, MODEL_NUM_LABELS)\n    ).to(device)\n    ensembling_audio_head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM, 256),\n        nn.ReLU(),\n        nn.Linear(256, MODEL_NUM_LABELS)\n    ).to(device)\n    ensembling_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=4, dim_feedforward=2048, batch_first=True).to(device)\n    ensembling_transformer_encoder = nn.TransformerEncoder(ensembling_transformer_layer, num_layers=1).to(device)\n    ensembling_text_model = TextModel(tokenizer, embedder, ensembling_text_head)\n    ensembling_audio_model = AudioModel(ensembling_transformer_encoder, ensembling_audio_head)\n    ensembling_fusion = EnsemblingFusion(ensembling_text_model, ensembling_audio_model).to(device)\n\n    ###################################################################################### -- UNALIGNED MODEL --\n\n    unaligned_head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM*2, 256),\n        nn.ReLU(),\n        nn.Linear(256, MODEL_NUM_LABELS)\n    ).to(device)\n    unaligned_mm_model = UnalignedMultimodalModel(embedding_dim=EMBEDDING_DIM, d_ffn=2048, n_blocks=4, head=unaligned_head).to(device)\n    \n    ######################################################################################-\n    \n    model_names = [ 'text_only', 'audio_only', 'multimodal', 'ensembling', 'unaligned']\n    models = [ text_only, audio_only, multimodal_transformer, ensembling_fusion, unaligned_mm_model ]\n    \n    return model_names, models","metadata":{"execution":{"iopub.status.busy":"2024-01-28T23:57:12.544401Z","iopub.execute_input":"2024-01-28T23:57:12.544844Z","iopub.status.idle":"2024-01-28T23:57:12.561333Z","shell.execute_reply.started":"2024-01-28T23:57:12.544810Z","shell.execute_reply":"2024-01-28T23:57:12.560230Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"SEEDS = [1]#, 42, 69, 420, 666]\n\nval_results = {\n    'text_only': [],\n    'audio_only': [],\n    'multimodal': [],\n    'ensembling': [],\n    'unaligned': []\n}\n\ntest_results = {\n    'text_only': [],\n    'audio_only': [],\n    'multimodal': [],\n    'ensembling': [],\n    'unaligned': []\n}\n\nEPOCHS = 3\nINITIAL_LR = 1e-3\nWEIGHT_DECAY = 1e-5\nLR_DECAY_FACTOR = 1e-1\nLR_DECAY_PATIENCE = 3\nDROPOUT = 0.1\nVERBOSE_TRAIN = True\n\nfor seed in SEEDS:\n    print(f'{f\"TRAINING WITH SEED {seed}\":=^65}')\n    print()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    model_names, models = create_models()\n    \n    # TO select a sinle model:\n    models = [models[3]]\n    model_names = [model_names[3]]\n\n    while models:\n        model = models[0]\n        model_name = model_names[0]\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        print(f'{f\"Training model {model_name}\":_^65}')\n        \n        loss = ce_loss\n        def custom_loss(outputs, targets):\n            return torch.nn.functional.nll_loss(torch.log(outputs), targets, reduction='mean')\n        \n        if model_name == 'ensembling':\n            loss = custom_loss\n            WEIGHT_DECAY = 1e-3\n            INITIAL_LR = 1e-4\n        \n        if model_name == 'audio_only':\n            WEIGHT_DECAY = 1e-3\n            INITIAL_LR = 1e-4\n            \n        train(\n            model,\n            loss,\n            train_dataloader,\n            val_dataloader,\n            epochs=EPOCHS,\n            device=device,\n            lr=INITIAL_LR,\n            lr_decay_factor=LR_DECAY_FACTOR,\n            lr_decay_patience=LR_DECAY_PATIENCE,\n            weight_decay=WEIGHT_DECAY,\n            verbose=VERBOSE_TRAIN,\n            debug = False\n        )\n\n        _, val_acc, val_f1, val_pred, val_targ = evaluate(model, val_dataloader, loss)\n        _, test_acc, test_f1, test_pred, test_targ = evaluate(model, test_dataloader, loss)\n        if VERBOSE_TRAIN:\n            print(f'[VAL] Model: {model_name} - acc: {val_acc:.4f} - f1: {val_f1:.4f}')\n            print(f'[TEST] Model: {model_name} - acc: {test_acc:.4f} - f1: {test_f1:.4f}')\n            print()\n        val_results[model_name].append({\n            'acc': val_acc,\n            'f1': val_f1,\n            'pred': val_pred,\n            'targ': val_targ\n        })\n        test_results[model_name].append({\n            'acc': test_acc,\n            'f1': test_f1,\n            'pred': test_pred,\n            'targ': test_targ\n        })\n        \n        del model\n        del models[0]\n        del model_names[0]\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T00:06:43.912453Z","iopub.execute_input":"2024-01-29T00:06:43.912913Z","iopub.status.idle":"2024-01-29T00:12:39.921794Z","shell.execute_reply.started":"2024-01-29T00:06:43.912855Z","shell.execute_reply":"2024-01-29T00:12:39.920638Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"======================TRAINING WITH SEED 1=======================\n\n____________________Training model ensembling____________________\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 1/3 [01:34<03:09, 94.85s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.0521, Validation Loss: 0.0512, accuracy = 0.6785, F1=0.6691\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 2/3 [03:10<01:35, 95.14s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.0487, Validation Loss: 0.0506, accuracy = 0.6789, F1=0.6771\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [04:45<00:00, 95.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.0482, Validation Loss: 0.0503, accuracy = 0.6841, F1=0.6800\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[VAL] Model: ensembling - acc: 0.6841 - f1: 0.6800\n[TEST] Model: ensembling - acc: 0.6791 - f1: 0.6767\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Error Analysis","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}