{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:58:35.751441Z","iopub.status.busy":"2024-01-28T17:58:35.751102Z","iopub.status.idle":"2024-01-28T17:58:49.363714Z","shell.execute_reply":"2024-01-28T17:58:49.362459Z","shell.execute_reply.started":"2024-01-28T17:58:35.751409Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torcheval\n","  Obtaining dependency information for torcheval from https://files.pythonhosted.org/packages/e4/de/e7abc784b00de9d05999657d29187f1f7a3406ed10ecaf164de06482608f/torcheval-0.0.7-py3-none-any.whl.metadata\n","  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.5.0)\n","Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torcheval\n","Successfully installed torcheval-0.0.7\n"]}],"source":["!pip install torcheval"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:58:49.366395Z","iopub.status.busy":"2024-01-28T17:58:49.366006Z","iopub.status.idle":"2024-01-28T17:59:06.641927Z","shell.execute_reply":"2024-01-28T17:59:06.640807Z","shell.execute_reply.started":"2024-01-28T17:58:49.366360Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import os\n","import gc\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import transformers\n","import torch, torchaudio, torchtext\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import warnings\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torcheval.metrics.functional import multiclass_f1_score\n","from transformers import BertTokenizer, BertModel, AutoModel, AutoProcessor\n","from tqdm import tqdm\n","from CustomTransformer import CustomEncoder\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["### Constants"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","MODEL_NUM_LABELS = 3\n","REMOVE_OTHER = True\n","OTHER_LABEL = 'O'\n","    \n","if REMOVE_OTHER:\n","    MODEL_NUM_LABELS = 2\n","\n","EMBEDDING_DIM = 768\n","BATCH_SIZE = 8"]},{"cell_type":"markdown","metadata":{},"source":["# Load df"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:06.643873Z","iopub.status.busy":"2024-01-28T17:59:06.643220Z","iopub.status.idle":"2024-01-28T17:59:07.031905Z","shell.execute_reply":"2024-01-28T17:59:07.031003Z","shell.execute_reply.started":"2024-01-28T17:59:06.643842Z"},"trusted":true},"outputs":[],"source":["try:\n","    # Try to load from Kaggle\n","    df_path = '/kaggle/input/multimodal-argument-mining/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n","    audio_path = '/kaggle/input/multimodal-argument-mining/MM-USElecDeb60to16/audio_clips'\n","    save_path = '/kaggle/input/mm-dataset-subsampling/'\n","    df = pd.read_csv(df_path, index_col=0)\n","except FileNotFoundError:\n","    # Try to load from local\n","    df_path = 'multimodal-dataset/files/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n","    audio_path = 'multimodal-dataset/files/MM-USElecDeb60to16/audio_clips'\n","    save_path = 'multimodal-dataset/files'\n","    df = pd.read_csv(df_path, index_col=0)\n","    \n","# drop rows where audio length is 0\n","df = df[df['NewBegin'] != df['NewEnd']]\n","if REMOVE_OTHER:\n","    # drop rows where Component is 'Other'\n","    df = df[df['Component'] != OTHER_LABEL]\n","\n","# train, val, test split\n","train_df_complete = df[df['Set'] == 'TRAIN']\n","val_df_complete = df[df['Set'] == 'VALIDATION']\n","test_df_complete = df[df['Set'] == 'TEST']\n","\n","# subsample datasets for memory reasons\n","DATASET_RATIO = 1\n","train_df = train_df_complete.iloc[:int(DATASET_RATIO * len(train_df_complete))]\n","val_df = val_df_complete.iloc[:int(DATASET_RATIO * len(val_df_complete))]\n","test_df = test_df_complete.iloc[:int(DATASET_RATIO * len(test_df_complete))]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:07.034043Z","iopub.status.busy":"2024-01-28T17:59:07.033742Z","iopub.status.idle":"2024-01-28T17:59:07.062781Z","shell.execute_reply":"2024-01-28T17:59:07.061907Z","shell.execute_reply.started":"2024-01-28T17:59:07.034006Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Part</th>\n","      <th>Document</th>\n","      <th>Order</th>\n","      <th>Sentence</th>\n","      <th>Start</th>\n","      <th>End</th>\n","      <th>Annotator</th>\n","      <th>Tag</th>\n","      <th>Component</th>\n","      <th>...</th>\n","      <th>Speaker</th>\n","      <th>SpeakerType</th>\n","      <th>Set</th>\n","      <th>Date</th>\n","      <th>Year</th>\n","      <th>Name</th>\n","      <th>MainTag</th>\n","      <th>NewBegin</th>\n","      <th>NewEnd</th>\n","      <th>idClip</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>And, after 9/11, it became clear that we had t...</td>\n","      <td>1</td>\n","      <td>30_2004</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2418</td>\n","      <td>2744</td>\n","      <td>NaN</td>\n","      <td>{\"O\": 16, \"Claim\": 50}</td>\n","      <td>Claim</td>\n","      <td>...</td>\n","      <td>CHENEY</td>\n","      <td>Candidate</td>\n","      <td>TRAIN</td>\n","      <td>05 Oct 2004</td>\n","      <td>2004</td>\n","      <td>Richard(Dick) B. Cheney</td>\n","      <td>Claim</td>\n","      <td>140.56</td>\n","      <td>158.92</td>\n","      <td>clip_3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>And we also then finally had to stand up democ...</td>\n","      <td>1</td>\n","      <td>30_2004</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2744</td>\n","      <td>2974</td>\n","      <td>NaN</td>\n","      <td>{\"O\": 4, \"Claim\": 13, \"Premise\": 25}</td>\n","      <td>Premise</td>\n","      <td>...</td>\n","      <td>CHENEY</td>\n","      <td>Candidate</td>\n","      <td>TRAIN</td>\n","      <td>05 Oct 2004</td>\n","      <td>2004</td>\n","      <td>Richard(Dick) B. Cheney</td>\n","      <td>Mixed</td>\n","      <td>158.92</td>\n","      <td>172.92</td>\n","      <td>clip_4</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>What we did in Iraq was exactly the right thin...</td>\n","      <td>1</td>\n","      <td>30_2004</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>3861</td>\n","      <td>3916</td>\n","      <td>NaN</td>\n","      <td>{\"Claim\": 12, \"O\": 1}</td>\n","      <td>Claim</td>\n","      <td>...</td>\n","      <td>CHENEY</td>\n","      <td>Candidate</td>\n","      <td>TRAIN</td>\n","      <td>05 Oct 2004</td>\n","      <td>2004</td>\n","      <td>Richard(Dick) B. Cheney</td>\n","      <td>Claim</td>\n","      <td>224.08</td>\n","      <td>226.88</td>\n","      <td>clip_9</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>If I had it to recommend all over again, I wou...</td>\n","      <td>1</td>\n","      <td>30_2004</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>3916</td>\n","      <td>4010</td>\n","      <td>NaN</td>\n","      <td>{\"Premise\": 19, \"O\": 1}</td>\n","      <td>Premise</td>\n","      <td>...</td>\n","      <td>CHENEY</td>\n","      <td>Candidate</td>\n","      <td>TRAIN</td>\n","      <td>05 Oct 2004</td>\n","      <td>2004</td>\n","      <td>Richard(Dick) B. Cheney</td>\n","      <td>Premise</td>\n","      <td>226.88</td>\n","      <td>231.56</td>\n","      <td>clip_10</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>The world is far safer today because Saddam Hu...</td>\n","      <td>1</td>\n","      <td>30_2004</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>4010</td>\n","      <td>4112</td>\n","      <td>NaN</td>\n","      <td>{\"Claim\": 6, \"O\": 2, \"Premise\": 13}</td>\n","      <td>Premise</td>\n","      <td>...</td>\n","      <td>CHENEY</td>\n","      <td>Candidate</td>\n","      <td>TRAIN</td>\n","      <td>05 Oct 2004</td>\n","      <td>2004</td>\n","      <td>Richard(Dick) B. Cheney</td>\n","      <td>Mixed</td>\n","      <td>231.56</td>\n","      <td>237.56</td>\n","      <td>clip_11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>"],"text/plain":["                                                 Text  Part Document  Order  \\\n","3   And, after 9/11, it became clear that we had t...     1  30_2004      3   \n","4   And we also then finally had to stand up democ...     1  30_2004      4   \n","9   What we did in Iraq was exactly the right thin...     1  30_2004      9   \n","10  If I had it to recommend all over again, I wou...     1  30_2004     10   \n","11  The world is far safer today because Saddam Hu...     1  30_2004     11   \n","\n","    Sentence  Start   End  Annotator                                   Tag  \\\n","3          3   2418  2744        NaN                {\"O\": 16, \"Claim\": 50}   \n","4          4   2744  2974        NaN  {\"O\": 4, \"Claim\": 13, \"Premise\": 25}   \n","9          9   3861  3916        NaN                 {\"Claim\": 12, \"O\": 1}   \n","10        10   3916  4010        NaN               {\"Premise\": 19, \"O\": 1}   \n","11        11   4010  4112        NaN   {\"Claim\": 6, \"O\": 2, \"Premise\": 13}   \n","\n","   Component  ... Speaker SpeakerType    Set         Date  Year  \\\n","3      Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n","4    Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n","9      Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n","10   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n","11   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n","\n","                       Name  MainTag NewBegin  NewEnd   idClip  \n","3   Richard(Dick) B. Cheney    Claim   140.56  158.92   clip_3  \n","4   Richard(Dick) B. Cheney    Mixed   158.92  172.92   clip_4  \n","9   Richard(Dick) B. Cheney    Claim   224.08  226.88   clip_9  \n","10  Richard(Dick) B. Cheney  Premise   226.88  231.56  clip_10  \n","11  Richard(Dick) B. Cheney    Mixed   231.56  237.56  clip_11  \n","\n","[5 rows x 21 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:07.064625Z","iopub.status.busy":"2024-01-28T17:59:07.064220Z","iopub.status.idle":"2024-01-28T17:59:07.071886Z","shell.execute_reply":"2024-01-28T17:59:07.070813Z","shell.execute_reply.started":"2024-01-28T17:59:07.064577Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(9455, 5908, 5201)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["len(train_df), len(test_df), len(val_df)"]},{"cell_type":"markdown","metadata":{},"source":["## Distribution of classes over train df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:22.036642Z","iopub.status.busy":"2024-01-28T17:59:22.036296Z","iopub.status.idle":"2024-01-28T17:59:22.051042Z","shell.execute_reply":"2024-01-28T17:59:22.050053Z","shell.execute_reply.started":"2024-01-28T17:59:22.036614Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Claim: 5029: 53.19%\n","Total Premise: 4426: 46.81%\n"]}],"source":["num_claim = len(train_df[train_df['Component'] == 'Claim'])\n","print(f'Total Claim: {num_claim}: {num_claim*100/len(train_df):.2f}%')\n","\n","num_premise = len(train_df[train_df['Component'] == 'Premise'])\n","print(f'Total Premise: {num_premise}: {num_premise*100/len(train_df):.2f}%')\n","\n","if not REMOVE_OTHER:\n","    num_other = len(train_df[train_df['Component'] == 'O'])\n","    print(f'Total Other: {num_other}: {num_other*100/len(train_df):.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["Classes are not balanced, but not too bad either."]},{"cell_type":"markdown","metadata":{},"source":["# Train and evaluation Loop"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:23.861840Z","iopub.status.busy":"2024-01-28T17:59:23.861209Z","iopub.status.idle":"2024-01-28T17:59:23.879197Z","shell.execute_reply":"2024-01-28T17:59:23.878221Z","shell.execute_reply.started":"2024-01-28T17:59:23.861812Z"},"trusted":true},"outputs":[],"source":["ce_loss = nn.CrossEntropyLoss()\n","\n","class BestModel:\n","    \"\"\"\n","    Class to keep track of the best performing model on validation set during training\n","    \"\"\"\n","    def __init__(self):\n","        self.best_validation_loss = float('Infinity')\n","        self.best_state_dict = None\n","    def __call__(self, model, loss):\n","        if loss < self.best_validation_loss:\n","            self.best_validation_loss = loss\n","            self.best_state_dict = model.state_dict()\n","\n","def evaluate(model, data_loader, loss_fn):\n","    \"\"\"\n","    Evaluate the model on the set passed\n","    Args:\n","        model: model to evaluate\n","        data_loader: DataLoader object\n","        loss_fn: loss function to use\n","    \"\"\"\n","    model.eval()\n","    valid_loss = 0.0\n","    num_correct = 0 \n","    num_examples = 0\n","    tot_pred, tot_targ = torch.LongTensor().to(device), torch.LongTensor().to(device)\n","    for batch in data_loader:\n","        texts, audio_features, audio_attention, targets = batch\n","        audio_features = audio_features.to(device)\n","        audio_attention = audio_attention.to(device)\n","        targets = targets.to(device)\n","        output = model(texts,audio_features,audio_attention)\n","        loss = loss_fn(output, targets)\n","        valid_loss += loss.detach()\n","        \n","        # if label O is still in the dataset we remove it from the outputs\n","        # since it's a binary task\n","        if not REMOVE_OTHER:\n","            not_other = targets != 2\n","            output = output[not_other]\n","            targets = targets[not_other]\n","        \n","        predicted_labels = torch.argmax(output[:, :2], dim=-1)\n","        tot_targ = torch.cat((tot_targ, targets))\n","        tot_pred = torch.cat((tot_pred, predicted_labels))            \n","        correct = torch.eq(predicted_labels, targets).view(-1)\n","        num_correct += torch.sum(correct).item()\n","        num_examples += correct.shape[0]\n","    valid_loss = valid_loss.cpu().item()\n","    valid_loss /= len(data_loader.dataset)\n","    accuracy = num_correct/num_examples\n","    f1 = multiclass_f1_score(tot_pred, tot_targ, num_classes=2, average=\"macro\")\n","    return valid_loss, accuracy, f1, tot_pred, tot_targ\n","\n","            \n","def train(model, loss_fn, train_loader, val_loader, epochs=10, device=\"cuda\", lr=1e-3, lr_decay_factor=0.1, lr_decay_patience=3, weight_decay=1e-5, verbose=True):\n","    \"\"\"\n","    Train the model on the train set and evaluate on the validation set with the given parameters\n","    Args:\n","        model: model to train\n","        loss_fn: loss function to use\n","        train_loader: DataLoader object for train set\n","        val_loader: DataLoader object for validation set\n","        epochs: number of epochs\n","        device: device to use\n","        lr: initial learning rate\n","        lr_decay_factor: factor to decay learning rate\n","        lr_decay_patience: patience for learning rate decay\n","        weight_decay: weight decay\n","    \"\"\"\n","    # set up optimizer and scheduler\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) \n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_decay_patience, verbose=True)\n","    best_model_tracker = BestModel()\n","    for epoch in tqdm(range(epochs)):\n","        training_loss = 0.0\n","        model.train()\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            texts, audio_features, audio_attention, targets = batch\n","            audio_features = audio_features.to(device)\n","            audio_attention = audio_attention.to(device)\n","            targets = targets.to(device)\n","            output = model(texts,audio_features,audio_attention)\n","            loss = loss_fn(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","            training_loss += loss.detach()\n","        training_loss = training_loss.cpu().item()\n","        training_loss /= len(train_loader.dataset)\n","        valid_loss, accuracy, f1, _, _ = evaluate(model, val_loader, loss_fn)\n","        best_model_tracker(model, valid_loss)\n","        scheduler.step(valid_loss)\n","        if verbose:\n","            print(f'Epoch: {epoch}, Training Loss: {training_loss:.4f}, Validation Loss: {valid_loss:.4f}, accuracy = {accuracy:.4f}, F1={f1:.4f}')\n","    model.load_state_dict(best_model_tracker.best_state_dict)    "]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Creation"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:30.226740Z","iopub.status.busy":"2024-01-28T17:59:30.226021Z","iopub.status.idle":"2024-01-28T17:59:33.852205Z","shell.execute_reply":"2024-01-28T17:59:33.851402Z","shell.execute_reply.started":"2024-01-28T17:59:30.226708Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f343f528baaa4fc8b6282be5eee48bfe","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b47bacdf8ee14f39ab18ec9728cda299","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68692d2e1bf64805a03d908371f7c462","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfd9b80be0eb4de1b64057b3dc064265","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"322c322b52284e28b228ee03c9bf76a8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# set up tokenizer and model\n","text_model_card = 'bert-base-uncased'\n","audio_model_card = 'facebook/wav2vec2-base-960h'\n","\n","tokenizer = BertTokenizer.from_pretrained(text_model_card)\n","embedder = BertModel.from_pretrained(text_model_card).to(device)\n","\n","# freeze bert layers\n","for params in embedder.parameters():\n","    params.requires_grad = False\n","\n","label_2_id = {\n","    'Claim': 0,\n","    'Premise': 1,\n","    'O': 2\n","}\n","\n","# Downsample audio features to 1/5 of the original size to fit in memory\n","DOWNSAMPLE_FACTOR = 1/5\n","\n","class MM_Dataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Dataset class for multimodal dataset\n","    \"\"\"\n","    def __init__(self, df, audio_dir, sample_rate):\n","        \"\"\"\n","        Args:\n","            df: dataframe containing the dataset\n","            audio_dir: directory containing the audio clips\n","            sample_rate: sample rate to use for audio clips\n","        \"\"\"\n","        self.audio_dir = audio_dir\n","        self.sample_rate = sample_rate\n","\n","        self.audio_processor = AutoProcessor.from_pretrained(audio_model_card)\n","        self.audio_model = AutoModel.from_pretrained(audio_model_card).to(device)\n","\n","        self.dataset = []\n","\n","        # Iterate over df\n","        for _, row in tqdm(df.iterrows()):\n","            path = os.path.join(self.audio_dir, f\"{row['Document']}/{row['idClip']}.wav\")\n","            if os.path.exists(path):\n","                # obtain audio WAV2VEC features\n","                audio, sampling_rate = torchaudio.load(path)\n","                # resample audio if necessary\n","                if sampling_rate != self.sample_rate:\n","                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\n","                    # mean pooling over channels\n","                    audio = torch.mean(audio, dim=0, keepdim=True)\n","                with torch.inference_mode():\n","                    # run audio through model\n","                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\n","                    input_values = torch.tensor(input_values).to(device)\n","                    audio_model_output = self.audio_model(input_values)\n","                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\n","                    # downsample audio features\n","                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode='linear')\n","                    audio_features = audio_features.permute(0,2,1)[0]\n","                    audio_features = audio_features.cpu()\n","                \n","                text = row['Text']\n","\n","                self.dataset.append((text, audio_features, label_2_id[row['Component']]))\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, index):\n","        return self.dataset[index]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T17:59:35.074915Z","iopub.status.busy":"2024-01-28T17:59:35.073981Z","iopub.status.idle":"2024-01-28T18:02:56.478036Z","shell.execute_reply":"2024-01-28T18:02:56.477116Z","shell.execute_reply.started":"2024-01-28T17:59:35.074871Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Restored datasets from memory\n"]}],"source":["try:\n","    train_dataset = torch.load(f'{save_path}/train_dataset.pkl')\n","    test_dataset = torch.load(f'{save_path}/test_dataset.pkl')\n","    val_dataset = torch.load(f'{save_path}/val_dataset.pkl')\n","    if REMOVE_OTHER:\n","        train_dataset = list(filter(lambda x: x[2] != 2, train_dataset))\n","        test_dataset = list(filter(lambda x: x[2] != 2, test_dataset))\n","        val_dataset = list(filter(lambda x: x[2] != 2, val_dataset))\n","    print('Restored datasets from memory')\n","except:\n","    print('Creating new datasets')\n","    train_dataset = MM_Dataset(train_df, audio_path, 16_000)\n","    test_dataset = MM_Dataset(test_df, audio_path, 16_000)\n","    val_dataset = MM_Dataset(val_df, audio_path, 16_000)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataloader creation"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T18:03:08.276333Z","iopub.status.busy":"2024-01-28T18:03:08.275955Z","iopub.status.idle":"2024-01-28T18:03:08.284972Z","shell.execute_reply":"2024-01-28T18:03:08.284131Z","shell.execute_reply.started":"2024-01-28T18:03:08.276304Z"},"trusted":true},"outputs":[],"source":["def create_dataloader(dataset, batch_size):\n","    \"\"\"\n","    Create a DataLoader object from the given dataset with the given batch size\n","    Args:\n","        dataset: dataset to use\n","        batch_size: batch size to use\n","    \"\"\"\n","    def pack_fn(batch):\n","        \"\"\"\n","        Function to pad the audio features and create the attention mask\n","        \"\"\"\n","        texts = [x[0] for x in batch]\n","        audio_features = [x[1] for x in batch]\n","        labels = torch.tensor([x[2] for x in batch])\n","        \n","        # pad audio features\n","        audio_features = pad_sequence(audio_features, batch_first=True, padding_value=float('-inf'))\n","        audio_features_attention_mask = audio_features[:, :, 0] != float('-inf')\n","        audio_features[(audio_features == float('-inf'))] = 0\n","        return texts, audio_features, audio_features_attention_mask, labels\n","\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pack_fn)\n","    return dataloader"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T18:03:10.572426Z","iopub.status.busy":"2024-01-28T18:03:10.571480Z","iopub.status.idle":"2024-01-28T18:03:10.578056Z","shell.execute_reply":"2024-01-28T18:03:10.576989Z","shell.execute_reply.started":"2024-01-28T18:03:10.572389Z"},"trusted":true},"outputs":[],"source":["train_dataloader = create_dataloader(train_dataset, BATCH_SIZE)\n","val_dataloader = create_dataloader(val_dataset, BATCH_SIZE)\n","test_dataloader = create_dataloader(test_dataset, BATCH_SIZE)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T18:03:12.580088Z","iopub.status.busy":"2024-01-28T18:03:12.579719Z","iopub.status.idle":"2024-01-28T18:03:12.898848Z","shell.execute_reply":"2024-01-28T18:03:12.897890Z","shell.execute_reply.started":"2024-01-28T18:03:12.580047Z"},"trusted":true},"outputs":[{"data":{"text/plain":["18"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T18:03:15.309524Z","iopub.status.busy":"2024-01-28T18:03:15.309152Z","iopub.status.idle":"2024-01-28T18:03:15.314518Z","shell.execute_reply":"2024-01-28T18:03:15.313467Z","shell.execute_reply.started":"2024-01-28T18:03:15.309495Z"},"trusted":true},"outputs":[],"source":["def number_parameters(model):\n","    \"\"\"\n","    Computes the number of trainable parameters in the model\n","    \"\"\"\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{},"source":["# 0-A Text-Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TextModel(nn.Module):\n","    \"\"\"\n","    Class for the text-only model\n","    \"\"\"\n","    def __init__(self, tokenizer, embedder, head):\n","        \"\"\"\n","        Args:\n","            tokenizer: tokenizer to use\n","            embedder: embedder to use\n","            head: head to use\n","        \"\"\"\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.embedder = embedder\n","        self.head = head\n","    def forward(self, texts, audio_features, audio_attention):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            texts: texts to use\n","            audio_features: audio features to use\n","            audio_attentions: audio attentions to use\n","        \"\"\"\n","        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n","        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n","        text_features = embedder_output['last_hidden_state']\n","\n","        # pooling transformer output\n","        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n","        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n","        return self.head(text_features_pooled)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Baseline model using only text\n","\"\"\"\n","\n","text_only_head = nn.Sequential(\n","    nn.Linear(EMBEDDING_DIM, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, MODEL_NUM_LABELS)\n",").to(device)\n","\n","text_only = TextModel(tokenizer, embedder, text_only_head)\n","\n","train(text_only, ce_loss, train_dataloader, val_dataloader, epochs=20, device=device)\n","\n","test_loss, acc, f1, _, _ = evaluate(text_only, test_dataloader, ce_loss)\n","print('Results on Test Set: ')\n","print(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')"]},{"cell_type":"markdown","metadata":{},"source":["# 0-B Audio-Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AudioModel(nn.Module):        \n","    \"\"\"\n","    Class for the audio-only model\n","    \"\"\"\n","    def __init__(self, transformer, head):\n","        \"\"\"\n","        Args:\n","            transformer: transformer to use\n","            head: head to use\n","        \"\"\"\n","        super().__init__()\n","        self.pos_encoder = PositionalEncoding(768, dual_modality=False)\n","        self.transformer = transformer\n","        self.head = head\n","        \n","    def forward(self, texts, audio_features, audio_attention):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            texts: texts to use\n","            audio_features: audio features to use\n","            audio_attentions: audio attentions to use\n","        \"\"\"\n","        audio_attention = torch.ones_like(audio_attention)\n","        padding_mask = ~audio_attention.to(torch.bool)\n","        #full_attention_mask = torch.zeros((audio_features.shape[1],audio_features.shape[1]), dtype=torch.bool).to(device)\n","        audio_features = self.pos_encoder(audio_features)\n","        # TODO: look carefully at this part of the code\n","        transformer_output = self.transformer(src=audio_features, is_causal=True)\n","        \n","        # pooling transformer output\n","        transformer_output_sum = (transformer_output * audio_attention.unsqueeze(-1)).sum(axis=1)\n","        transformer_output_pooled = transformer_output_sum / audio_attention.sum(axis=1).unsqueeze(-1)\n","        return self.head(transformer_output_pooled)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Baseline model using only audio\n","\"\"\"\n","audio_only_head = nn.Sequential(\n","    nn.Linear(EMBEDDING_DIM, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, MODEL_NUM_LABELS)\n",").to(device)\n","\n","audio_only_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=4, dim_feedforward=512, batch_first=True).to(device)\n","audio_only_transformer_encoder = nn.TransformerEncoder(audio_only_transformer_layer, num_layers=4).to(device)\n","\n","audio_only = AudioModel(audio_only_transformer_encoder, audio_only_head).to(device)\n","print(f'#Params: {number_parameters(audio_only)}')\n","\n","train(audio_only, ce_loss, train_dataloader, val_dataloader, epochs=20, device=device, lr=1e-3)\n","\n","test_loss, acc, f1, _, _ = evaluate(audio_only, test_dataloader, ce_loss)\n","print('Results on Test Set: ')\n","print(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')"]},{"cell_type":"markdown","metadata":{},"source":["# 1 - Multimodal-Transformer"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T18:04:35.267918Z","iopub.status.busy":"2024-01-28T18:04:35.267051Z","iopub.status.idle":"2024-01-28T18:04:35.277246Z","shell.execute_reply":"2024-01-28T18:04:35.276244Z","shell.execute_reply.started":"2024-01-28T18:04:35.267888Z"},"trusted":true},"outputs":[],"source":["class MultiModalTransformer(nn.Module):\n","    \"\"\"\n","    Class for the multimodal transformer model\n","    \"\"\"\n","    def __init__(self, tokenizer, embedder, transformer, head):\n","        \"\"\"\n","        Args:\n","            tokenizer: tokenizer to use\n","            embedder: embedder to use\n","            transformer: transformer to use\n","            head: head to use\n","        \"\"\"\n","        super().__init__()\n","        self.pos_encoder = PositionalEncoding(EMBEDDING_DIM, dual_modality=False)\n","        self.tokenizer = tokenizer\n","        self.embedder = embedder\n","        self.transformer = transformer\n","        self.head = head\n","\n","    def forward(self, texts, audio_features, audio_attentions):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            texts: texts to use\n","            audio_features: audio features to use\n","            audio_attentions: audio attentions to use\n","        \"\"\"\n","        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n","        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n","        text_features = embedder_output['hidden_states'][0]\n","        text_attentions = tokenizer_output.attention_mask\n","\n","        concatenated_attentions = torch.cat((text_attentions, audio_attentions.float()), dim=1)\n","        \n","        audio_features = self.pos_encoder(audio_features)\n","        \n","        concatenated_features = torch.cat((text_features, audio_features), dim=1)\n","\n","        transformer_output = self.transformer(concatenated_features, text_attentions, audio_attentions)\n","\n","        # pooling of transformer output        \n","        transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1)\n","        transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1)\n","        return self.head(transformer_output_pooled)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T18:05:01.043892Z","iopub.status.busy":"2024-01-28T18:05:01.043564Z","iopub.status.idle":"2024-01-28T18:36:10.319042Z","shell.execute_reply":"2024-01-28T18:36:10.318113Z","shell.execute_reply.started":"2024-01-28T18:05:01.043868Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["#parameters: 5711362\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 1/20 [01:33<29:34, 93.39s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Training Loss: 0.0861, Validation Loss: 0.0836, accuracy = 0.5816, F1=0.5798\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 2/20 [03:05<27:47, 92.66s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Training Loss: 0.0815, Validation Loss: 0.0803, accuracy = 0.6406, F1=0.6393\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 3/20 [04:38<26:13, 92.59s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2, Training Loss: 0.0767, Validation Loss: 0.0818, accuracy = 0.6543, F1=0.6508\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 4/20 [06:10<24:41, 92.58s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3, Training Loss: 0.0743, Validation Loss: 0.0762, accuracy = 0.6685, F1=0.6640\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 5/20 [07:42<23:06, 92.41s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4, Training Loss: 0.0720, Validation Loss: 0.0753, accuracy = 0.6728, F1=0.6710\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 6/20 [09:14<21:29, 92.11s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5, Training Loss: 0.0704, Validation Loss: 0.0785, accuracy = 0.6683, F1=0.6666\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▌      | 7/20 [10:45<19:54, 91.90s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6, Training Loss: 0.0688, Validation Loss: 0.0767, accuracy = 0.6674, F1=0.6565\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 8/20 [12:16<18:19, 91.65s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7, Training Loss: 0.0670, Validation Loss: 0.0767, accuracy = 0.6718, F1=0.6698\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 9/20 [13:48<16:48, 91.72s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch: 8, Training Loss: 0.0655, Validation Loss: 0.0771, accuracy = 0.6737, F1=0.6654\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 10/20 [15:20<15:18, 91.84s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9, Training Loss: 0.0626, Validation Loss: 0.0784, accuracy = 0.6774, F1=0.6734\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▌    | 11/20 [16:51<13:44, 91.59s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10, Training Loss: 0.0604, Validation Loss: 0.0806, accuracy = 0.6806, F1=0.6770\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 12/20 [18:22<12:11, 91.43s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 11, Training Loss: 0.0597, Validation Loss: 0.0806, accuracy = 0.6795, F1=0.6772\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▌   | 13/20 [19:54<10:39, 91.38s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch: 12, Training Loss: 0.0586, Validation Loss: 0.0821, accuracy = 0.6770, F1=0.6752\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 14/20 [21:25<09:08, 91.33s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 13, Training Loss: 0.0579, Validation Loss: 0.0827, accuracy = 0.6781, F1=0.6759\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 15/20 [22:56<07:36, 91.35s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 14, Training Loss: 0.0574, Validation Loss: 0.0829, accuracy = 0.6804, F1=0.6780\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 16/20 [24:27<06:05, 91.29s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 15, Training Loss: 0.0573, Validation Loss: 0.0835, accuracy = 0.6795, F1=0.6771\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▌ | 17/20 [25:59<04:33, 91.27s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch: 16, Training Loss: 0.0573, Validation Loss: 0.0831, accuracy = 0.6797, F1=0.6772\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 18/20 [27:30<03:02, 91.17s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 17, Training Loss: 0.0572, Validation Loss: 0.0836, accuracy = 0.6799, F1=0.6774\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 19/20 [29:01<01:31, 91.22s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 18, Training Loss: 0.0574, Validation Loss: 0.0831, accuracy = 0.6801, F1=0.6776\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 20/20 [30:34<00:00, 91.72s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 19, Training Loss: 0.0571, Validation Loss: 0.0832, accuracy = 0.6797, F1=0.6772\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Results on Test Set: \n","Test loss: 0.08729851786639937\tAccuracy: 0.6541976980365606\tF1: 0.6528137922286987\n"]}],"source":["# TRAINING OF MULTIMODAL TRANSFORMER\n","multimodal_encoder = CustomEncoder(d_model=EMBEDDING_DIM, ffn_hidden=2048, n_head=4, n_layers=1, drop_prob=0.1)\n","\n","multimodal_transformer_head = nn.Sequential(\n","    nn.Linear(EMBEDDING_DIM, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, MODEL_NUM_LABELS)\n",").to(device)\n","\n","multimodal_transformer = MultiModalTransformer(tokenizer, embedder, multimodal_encoder, multimodal_transformer_head).to(device)\n","\n","multimodal_optimizer = torch.optim.Adam(multimodal_transformer.parameters(), lr=1e-3)\n","multimodal_criterion = nn.CrossEntropyLoss()\n","\n","print(f'#parameters: {number_parameters(multimodal_transformer)}')\n","\n","train(multimodal_transformer, multimodal_criterion, train_dataloader, val_dataloader, epochs=20, device=device)\n","\n","test_loss, acc, f1, _, _ = evaluate(multimodal_transformer, test_dataloader, ce_loss)\n","print('Results on Test Set: ')\n","print(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')"]},{"cell_type":"markdown","metadata":{},"source":["# 2 - Ensembling-Fusion"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:15.307078Z","iopub.status.busy":"2024-01-27T10:01:15.306743Z","iopub.status.idle":"2024-01-27T10:01:15.321688Z","shell.execute_reply":"2024-01-27T10:01:15.320753Z","shell.execute_reply.started":"2024-01-27T10:01:15.307054Z"},"trusted":true},"outputs":[],"source":[" class EnsemblingFusion(nn.Module):\n","    \"\"\"\n","    Class for the ensembling model\n","    \"\"\"\n","    def __init__(self, text_model, audio_model):\n","        \"\"\"\n","        Args:\n","            text_model: text model to use\n","            audio_model: audio model to use\n","        \"\"\"\n","        super().__init__()\n","        self.text_model = text_model\n","        self.audio_model = audio_model\n","        # weight to balance the two models\n","        self.weight = torch.nn.Parameter(torch.tensor(0.0))\n","        \n","    def forward(self, texts, audio_features, audio_attentions):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            texts: texts to use\n","            audio_features: audio features to use\n","            audio_attentions: audio attentions to use\n","        \"\"\"\n","        text_logits = self.text_model(texts, audio_features, audio_attentions)\n","        audio_logits = self.audio_model(texts, audio_features, audio_attentions)\n","        \n","        text_probabilities = torch.nn.functional.softmax(text_logits)\n","        audio_probabilities = torch.nn.functional.softmax(audio_logits)\n","        \n","        # coefficient to balance the two models based on weight learned\n","        # (tanh + 1) / 2 to have values in [0,1]\n","        coefficient = (torch.tanh(self.weight) + 1) / 2\n","        # next step is to have values in [0.3,0.7] to avoid too much imbalance\n","        coefficient = coefficient*0.4 + 0.3\n","        \n","        return coefficient*text_probabilities + (1-coefficient)*audio_probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T17:28:15.659871Z","iopub.status.busy":"2024-01-26T17:28:15.659450Z","iopub.status.idle":"2024-01-26T17:51:03.358581Z","shell.execute_reply":"2024-01-26T17:51:03.357647Z","shell.execute_reply.started":"2024-01-26T17:28:15.659813Z"},"trusted":true},"outputs":[],"source":["# TRAINING OF ENSEMBLING\n","ensembling_text_head = nn.Sequential(\n","    nn.Linear(EMBEDDING_DIM, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, MODEL_NUM_LABELS)\n",").to(device)\n","\n","ensembling_audio_head = nn.Sequential(\n","    nn.Linear(EMBEDDING_DIM, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, MODEL_NUM_LABELS)\n",").to(device)\n","\n","ensembling_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=4, dim_feedforward=512, batch_first=True).to(device)\n","ensembling_transformer_encoder = nn.TransformerEncoder(ensembling_transformer_layer, num_layers=4).to(device)\n","\n","ensembling_text_model = TextModel(tokenizer, embedder, ensembling_text_head)\n","ensembling_audio_model = AudioModel(ensembling_transformer_encoder, ensembling_audio_head)\n","\n","ensembling_fusion = EnsemblingFusion(ensembling_text_model, ensembling_audio_model).to(device)\n","\n","print(f'#Params: {number_parameters(ensembling_fusion)}')\n","def custom_loss(outputs, targets):\n","    return torch.nn.functional.nll_loss(torch.log(outputs), targets, reduction='mean')\n","\n","train(ensembling_fusion, custom_loss, train_dataloader, val_dataloader, epochs=20, device=device)\n","\n","test_loss, acc, f1, _, _ = evaluate(ensembling_fusion, test_dataloader, ce_loss)\n","print('Results on Test Set: ')\n","print(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')"]},{"cell_type":"markdown","metadata":{},"source":["# 3 - Unaligned Multimodal Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T18:20:49.118507Z","iopub.status.busy":"2024-01-26T18:20:49.117899Z","iopub.status.idle":"2024-01-26T18:20:49.134944Z","shell.execute_reply":"2024-01-26T18:20:49.134006Z","shell.execute_reply.started":"2024-01-26T18:20:49.118472Z"},"trusted":true},"outputs":[],"source":["class UnalignedPositionwiseFeedForward(nn.Module):\n","    \"\"\"\n","    Class for the positionwise feed forward layer\n","    \"\"\"\n","    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n","        \"\"\"\n","        Args:\n","            d_model: dimension of the model\n","            d_ffn: dimension of the feed forward layer\n","            dropout: dropout to use\n","        \"\"\"\n","        super().__init__()\n","        self.w_1 = nn.Linear(d_model, d_ffn)\n","        self.w_2 = nn.Linear(d_ffn, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            x: input to use\n","        \"\"\"\n","        return self.w_2(self.dropout(self.w_1(x).relu()))\n","\n","class CrossModalAttentionBlock(nn.Module):\n","    \"\"\"\n","    Class for the cross modal attention block\n","    \"\"\"\n","    def __init__(self, embedding_dim, d_ffn):\n","        \"\"\"\n","        Args:\n","            embedding_dim: dimension of the embedding\n","            d_ffn: dimension of the feed forward layer\n","        \"\"\"\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.d_ffn = d_ffn\n","        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n","        self.mh_attention = nn.MultiheadAttention(self.embedding_dim, 4, 0.1, batch_first=True)\n","        self.pointwise_ff = UnalignedPositionwiseFeedForward(self.embedding_dim, d_ffn=self.d_ffn)\n","    \n","    def forward(self, elem_a, elem_b, attn_mask):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            elem_a: elements of the modality A\n","            elem_b: elements of the modality B\n","            attn_mask: attention mask to use\n","        \"\"\"\n","        elem_a = self.layer_norm(elem_a)\n","        elem_b = self.layer_norm(elem_b)\n","        attn_mask = attn_mask.to(torch.float32)\n","        \n","        # cross modal attention with elem_a as query and elem_b as key and value\n","        mh_out, _ = self.mh_attention(elem_a, elem_b, elem_b, key_padding_mask=attn_mask, need_weights=False)\n","        # residual connection\n","        add_out = mh_out + elem_a\n","        \n","        add_out_norm = self.layer_norm(add_out)\n","        out_ffn = self.pointwise_ff(add_out_norm)\n","        out = out_ffn + add_out\n","        return out\n","    \n","class UnalignedMultimodalModel(nn.Module):\n","    \"\"\"\n","    Class for the unaligned multimodal model\n","    \"\"\"\n","    def __init__(self, embedding_dim, d_ffn, n_blocks, head):\n","        \"\"\"\n","        Args:\n","            embedding_dim: dimension of the embedding\n","            d_ffn: dimension of the feed forward layer\n","            n_blocks: number of blocks to use\n","            head: head to use\n","        \"\"\"\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.d_ffn = d_ffn\n","        self.n_blocks = n_blocks\n","        self.head = head\n","        self.text_crossmodal_blocks = nn.ModuleList([\n","            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n","        ])\n","        self.audio_crossmodal_blocks = nn.ModuleList([\n","            CrossModalAttentionBlock(self.embedding_dim, self.d_ffn) for _ in range(self.n_blocks)\n","        ])\n","        self.pos_encoder = PositionalEncoding(embedding_dim, dual_modality=False)\n","    \n","    def forward(self, texts, audio_features, audio_attentions):\n","        \"\"\"\n","        Forward pass of the model\n","        Args:\n","            texts: texts to use\n","            audio_features: audio features to use\n","            audio_attentions: audio attentions to use\n","        \"\"\"\n","        tokenizer_output = tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n","        embedder_output = embedder(**tokenizer_output, output_hidden_states=True)\n","        text_features = embedder_output['hidden_states'][0]\n","        text_features = self.pos_encoder(text_features)\n","        text_attentions = tokenizer_output.attention_mask\n","        \n","        audio_features = self.pos_encoder(audio_features)\n","        \n","        # cross modal attention blocks for text\n","        # using audio features as key and value and text features as query\n","        text_crossmodal_out = text_features\n","        for cm_block in self.text_crossmodal_blocks:\n","            text_crossmodal_out = cm_block(text_crossmodal_out, audio_features, audio_attentions)\n","        \n","        # cross modal attention blocks for audio\n","        # using text features as key and value and audio features as query\n","        audio_crossmodal_out = audio_features\n","        for cm_block in self.audio_crossmodal_blocks:\n","            audio_crossmodal_out = cm_block(audio_crossmodal_out, text_features, text_attentions)\n","\n","        # pooling of transformer output\n","        text_crossmodal_out_mean = torch.mean(text_crossmodal_out, dim=1)\n","        audio_crossmodal_out_mean = torch.mean(audio_crossmodal_out, dim=1)\n","        \n","        # concatenate text and audio features\n","        text_audio = torch.cat((text_crossmodal_out_mean, audio_crossmodal_out_mean), dim=-1)\n","        \n","        return self.head(text_audio)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T18:21:20.676638Z","iopub.status.busy":"2024-01-26T18:21:20.675779Z","iopub.status.idle":"2024-01-26T19:04:17.823914Z","shell.execute_reply":"2024-01-26T19:04:17.822964Z","shell.execute_reply.started":"2024-01-26T18:21:20.676597Z"},"trusted":true},"outputs":[],"source":["# TRAINING OF UNALIGNED-MODEL\n","unaligned_head = nn.Sequential(\n","    nn.Linear(EMBEDDING_DIM*2, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, MODEL_NUM_LABELS)\n",").to(device)\n","\n","unaligned_mm_model = UnalignedMultimodalModel(embedding_dim=EMBEDDING_DIM, d_ffn=100, n_blocks=4, head=unaligned_head).to(device)\n","\n","train(unaligned_mm_model, ce_loss, train_dataloader, val_dataloader, epochs=20, device=device)\n","\n","test_loss, acc, f1, _, _ = evaluate(unaligned_mm_model, test_dataloader, ce_loss)\n","print('Results on Test Set: ')\n","print(f'Test loss: {test_loss}\\tAccuracy: {acc}\\tF1: {f1}')"]},{"cell_type":"markdown","metadata":{},"source":["# Training of the models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_models(){\n","    \"\"\"\n","    Creates all the models\n","    \"\"\"\n","    # creating text-only model\n","    text_only_head = nn.Sequential(\n","        nn.Linear(EMBEDDING_DIM, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, MODEL_NUM_LABELS)\n","    ).to(device)\n","    text_only = TextModel(tokenizer, embedder, text_only_head)\n","\n","    # creating audio-only model\n","    audio_only_head = nn.Sequential(\n","        nn.Linear(EMBEDDING_DIM, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, MODEL_NUM_LABELS)\n","    ).to(device)\n","    audio_only_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=4, dim_feedforward=512, batch_first=True).to(device)\n","    audio_only_transformer_encoder = nn.TransformerEncoder(audio_only_transformer_layer, num_layers=4).to(device)\n","    audio_only = AudioModel(audio_only_transformer_encoder, audio_only_head).to(device)\n","\n","    # creating multimodal model\n","    multimodal_encoder = CustomEncoder(d_model=EMBEDDING_DIM, ffn_hidden=2048, n_head=4, n_layers=1, drop_prob=0.1)\n","    multimodal_transformer_head = nn.Sequential(\n","        nn.Linear(EMBEDDING_DIM, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, MODEL_NUM_LABELS)\n","    ).to(device)\n","    multimodal_transformer = MultiModalTransformer(tokenizer, embedder, multimodal_encoder, multimodal_transformer_head).to(device)\n","\n","    # creating ensembling model\n","    ensembling_text_head = nn.Sequential(\n","        nn.Linear(EMBEDDING_DIM, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, MODEL_NUM_LABELS)\n","    ).to(device)\n","    ensembling_audio_head = nn.Sequential(\n","        nn.Linear(EMBEDDING_DIM, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, MODEL_NUM_LABELS)\n","    ).to(device)\n","    ensembling_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=4, dim_feedforward=512, batch_first=True).to(device)\n","    ensembling_transformer_encoder = nn.TransformerEncoder(ensembling_transformer_layer, num_layers=4).to(device)\n","    ensembling_text_model = TextModel(tokenizer, embedder, ensembling_text_head)\n","    ensembling_audio_model = AudioModel(ensembling_transformer_encoder, ensembling_audio_head)\n","    ensembling_fusion = EnsemblingFusion(ensembling_text_model, ensembling_audio_model).to(device)\n","\n","    # creating unaligned multimodal model\n","    unaligned_head = nn.Sequential(\n","        nn.Linear(EMBEDDING_DIM*2, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, MODEL_NUM_LABELS)\n","    ).to(device)\n","    unaligned_mm_model = UnalignedMultimodalModel(embedding_dim=EMBEDDING_DIM, d_ffn=100, n_blocks=4, head=unaligned_head).to(device)\n","    return {\n","        'text_only': text_only,\n","        'audio_only': audio_only,\n","        'multimodal': multimodal_transformer,\n","        'ensembling': ensembling_fusion,\n","        'unaligned': unaligned_mm_model\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SEEDS = [0, 42, 69, 420, 666]\n","\n","val_results = {\n","    'text_only': [],\n","    'audio_only': [],\n","    'multimodal': [],\n","    'ensembling': [],\n","    'unaligned': []\n","}\n","\n","test_results = {\n","    'text_only': [],\n","    'audio_only': [],\n","    'multimodal': [],\n","    'ensembling': [],\n","    'unaligned': []\n","}\n","\n","EPOCHS = 20\n","INITIAL_LR = 1e-3\n","WEIGHT_DECAY = 1e-5\n","LR_DECAY_FACTOR = 1e-1\n","LR_DECAY_PATIENCE = 3\n","DROPOUT = 0.1\n","VERBOSE_TRAIN = True\n","\n","for seed in SEEDS:\n","    print(f'{f\"TRAINING WITH SEED {seed}\":=^65}')\n","    print()\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    # TODO: copilot suggested this 2 lines, check if they are useful\n","    # torch.backends.cudnn.deterministic = True\n","    # torch.backends.cudnn.benchmark = False\n","    \n","    models = create_models()\n","\n","    for model_name, model in models.items():\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        print(f'{f\"Training model {model_name}\":_^65}')\n","        loss = ce_loss\n","        if model_name == 'ensembling':\n","            loss = custom_loss\n","        train(\n","            model,\n","            loss,\n","            train_dataloader,\n","            val_dataloader,\n","            epochs=EPOCHS,\n","            device=device,\n","            lr=INITIAL_LR,\n","            lr_decay_factor=LR_DECAY_FACTOR,\n","            lr_decay_patience=LR_DECAY_PATIENCE,\n","            weight_decay=WEIGHT_DECAY,\n","            verbose=VERBOSE_TRAIN\n","        )\n","        _, val_acc, val_f1, val_pred, val_targ = evaluate(model, val_dataloader, loss)\n","        _, test_acc, test_f1, test_pred, test_targ = evaluate(model, test_dataloader, loss)\n","        if VERBOSE_TRAIN:\n","            print(f'[VAL] Model: {model_name} - acc: {val_acc:.4f} - f1: {val_f1:.4f}')\n","            print(f'[TEST] Model: {model_name} - acc: {test_acc:.4f} - f1: {test_f1:.4f}')\n","        val_results[model_name].append({\n","            'acc': val_acc,\n","            'f1': val_f1,\n","            'pred': val_pred,\n","            'targ': val_targ\n","        })\n","        test_results[model_name].append({\n","            'acc': test_acc,\n","            'f1': test_f1,\n","            'pred': test_pred,\n","            'targ': test_targ\n","        })"]},{"cell_type":"markdown","metadata":{},"source":["# Error Analysis"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4304373,"sourceId":7402218,"sourceType":"datasetVersion"},{"datasetId":4354582,"sourceId":7480566,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
